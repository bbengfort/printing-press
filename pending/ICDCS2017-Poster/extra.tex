\section{Latency}

Latency ranges for wide area networks can be extremely variable and are primarily determined by the last mile connection. While data centers often use backbone links to connect across the wide area, LTE networks, satellite networks, and the cable or fiber networks that connect users introduce extra round trip latency. Using ICMP to measure latency across the continental United States from the Princeton University network, Katz-Bassett et. al report local round trip latencies in the 10-50ms range and cross-country latencies in the 100ms range \cite{katz-bassett_towards_2006}. However, we believe that ICMP often gets preferential routing and that the simplicity of the echo protocol does not inform true message delays that would be experienced by a distributed system. We compared ICMP to a simple echo protocol implemented with GRPC and discovered that the ICMP latency distribution is significantly smaller than GRPC in many environments.

\section{Conclusions}

Our experiments utilized a very specific topology, and a natural question arises: what are the effects of different topologies on the performance of the system? Consider the case where we scale the number of eventually consistent nodes in each local area. More eventually consistent replicas would increase the anti-entropy propagation delay, increasing the likelihood of forks and stale reads and decreasing the number of writes that are fully replicated. The Raft core group could minimize the effect of this scaling to, however if the core Raft group remained fixed, it would also increase the load on the quorum for handling remote writes from the eventual cloud and there would be increasingly more drops, particularly as forks are propagated around Raft followers. However, the increase in the size of the eventual cloud would also give the system more fault tolerance as there are more paths to propagate updates when the Raft quorum is down, particularly across the wide area. Local outages may bring down Raft (partitioning the leader) but anti-entropy is resilient.

The failure mode that we considered took down wide area connections, if instead the failure mode was random node failure, the system would respond differently. In a quorum size of 5, Raft can handle 2 failures before an extended outage. Leader failure would cause temporary outages until the election timeout occurs, but at least one append entries message will be missed. The central Raft group is therefore the most susceptible part of the system to random node failure. If the Raft node that fails is a follower, then the eventually consistent nodes in that area can still make progress without a connection to the central quorum. Wide-area anti-entropy will allow updates to be propagated to the entire network without the central broadcast mechanism. However, the outage would probably lead to an increase in the number of forks as reads become increasingly stale with missed pairwise anti-entropy sessions reducing propagation speed.

In order to scale the topology and to increase the amount of failure tolerated, the central Raft quorum must also scale. However, increased quorum sizes often lead to decreased performance because the leader becomes a bottleneck as the number of messages increases. Additionally, the Raft quorum in our topology does not benefit from any localization like the eventually consistent nodes do. It is possible that placing all of the Raft nodes in a single location, giving Raft RPC messages the benefit of the local area connections and penalizing the synchronization of wide area eventual nodes, would have increased the overall performance of the system. Because of the centrality of the Raft quorum in providing stronger consistency guarantees in a federated environment, we propose future work into optimizing coordination for user-centric dynamic networks. In particular, we propose hierarchical consensus that will allow Raft to maintain smaller quorum sizes but scale to increasingly larger systems as well as localize decision making. 
//=====================================================================
Consistency in a distributed storage system is usually thought of as one of a few discrete categories: eventual, causal, sequential, or linearizable and distributed systems are designed with homogenous replica types to fit one of those categories. In this paper we present Federated Consistency, a heterogenous system model that allows individual replica servers to select their own consistency level, specifying a quality of service at a particular location while still providing global guarantees. This model, applied to variable latency geographic systems that are partition prone allow for flexibility in consistency that can adapt in response to the local network environment.
//=====================================================================

\subsection{Weak Consistency}

A weakly consistent system makes no guarantees whatsoever about the relationship of local vs. remote writes and whether or not any given update will become visible \cite{vogels_eventually_2009}. Weak consistency is often described as ``replicas might get lucky and become consistent'' and in fact a weakly consistent implementation may not have a synchronization protocol whatsoever \cite{bermbach_consistency_2013}.

Weakly consistent systems may have a role in a Federated environment, however. Consider a simple weak consistency synchronization protocol where any remote updates are accepted so long as they do not conflict with any local updates. This model may be used in the case of a replica that expects to be intermittently offline or unable to perform any conflict resolution. By accepting non-conflicting updates the replica can keep up as much as possible with the rest of the system while protecting a core set of objects that the device will routinely access. For example, in mobile mesh sensor networks, each sensor will be frequently writing to its own objects, updating their state and generally not conflicting with the measurements recorded by other sensors. A weakly consistent system may assists in propagating updates from remote sensors back to a core replica system that implements stronger consistency.

\subsection{Causal Consistency}

Causal consistency provides the strictest ordering guarantee without requiring coordination or consensus \cite{ladin_providing_1992}. In causal consistency all writes that have causal relationships must have those dependencies satisfied, e.g. inserted into the log before that write can become visible. Therefore, even though a write might have been propagated to another replica server it cannot be read until all of its dependencies have also been propagated. Causal consistency can increase staleness particularly when implicit or potential causality creates large dependency graphs that must be resolved before writes can be applied \cite{lloyd_dont_2011}. This can be managed by allowing the application to explicitly specify the dependencies for each write \cite{bailis_potential_2012}.

Our consistency model provides for causality by ensuring that dependencies are tracked along with specific writes to specific versions of objects; in fact every write has at least one dependency, the parent version of the write. Eventual consistency however does not require that the parent version be appended to the log before the write can be made visible, a requirement for causal. Moreover, parent versions only track causality for a single object and do not consider potential causality or ordering of writes to all objects in the namespace. As a result, in order to maintain inter-object causality each write must explicitly specify its dependencies.

\subsection{Linearizability}

Linearizability is the strongest form of consistency; not only must all write operations occur in sequence, but all operations including reads must be ordered chronologically \cite{herlihy_linearizability:_1990}. A consensus algorithm alone cannot implement linearizability and instead some distributed locking mechanism is required. For example a consensus algorithm can be adapted to instead of making decisions about the total ordering of conflicting writes, granting or releasing locks from requestors, however this opens up the potential for deadlock and extremely poor performance, defeating the purposes of replication in the first place! Data center environments that don't have to deal with issues of clock skew by using super precise atomic and GPS clocks can use precise time measurements to enable a distributed two phase commit protocol \cite{corbett_spanner:_2013}, however every replica is required to have such a time piece, which is not practical for heterogenous topologies.

//=====================================================================

\item \textbf{Stentor}: A ``Stentor'' node is an eventual node that performs \textit{two} anti-entropy sessions per interval, always choosing one local and one remote node. We have implemented Stentor nodes to show that Raft isn't simply acting as a minimum spanning tree in the network, but is also preserving consistency guarantees.

  //=====================================================================
  
The rise of virtualized, on-demand computing resources and cloud
computing has dramatically shifted the focus of research on
consistency in distributed storage systems away from networked file
systems toward geographically distributed database management systems,
particularly key-value storage.
These types of data systems benefit from low-latency, high-bandwidth
connections where failure is common due to the magnitude of resources
rather than inherent instability.
In a low latency environment, ``eventually consistent'' (EC)
\cite{vogels_eventually_2009} systems are said to be consistent enough
for most workloads given some probabilistic bounding of staleness
\cite{bailis_quantifying_2014,bermbach_metastorage:_2011,pbft}.
This has led consistency
research~\cite{bailis2013highly,bailis2014coordination,mahajan2011consistency,alvaro2013consistency,zawirski2015write}
towards investigating stronger forms of eventual consistency,
primarily building upon Dynamo \cite{decandia_dynamo:_2007} and
Cassandra \cite{lakshman_cassandra:_2010} as reference
implementations.
Alternatively, research into ensuring strong, sequential consistency
(SC)~\cite{sequential-consistency} or linearizability
(LIN)~\cite{herlihy_linearizability:_1990} in this environment has led
to systems like Spanner \cite{spanner}, which uses atomic and GPS
clocks to ensure ``TrueTime'' for sequential ordering, or a few master
nodes that implement consensus algorithms \cite{paxos} for locking or
ordering \cite{kraska_mdcc:_2013}.

The focus on data center consistency has led to a centralized approach to data storage,
forcing a modality where devices must connect to the cloud for file replication even when
files exist in the local area \cite{drago_inside_2012}.
Although this allows systems to ignore annoyances such as local network configuration and
decentralized protocols it does present unnecessary cost and latency.
More pernicious, perhaps, is that users must now buy-in and store their data with a single
provider that could go out of business or be hacked, which has lead to research in
replicating local data with multiple untrusted cloud stores
\cite{zhang_viewbox:_2014,sporc}.
There are more devices than ever before connecting to storage applications, partially
because users have multiple, heterogeneous devices from wearables to workstations and
partially because of the advent of the Internet of Things \cite{miorandi_internet_2012}.
Cloud storage tends to be application specific, but with more devices and more users,
generalized distributed storage is required for collaboration and sharing that is
not siloed and can be easily accessed by a variety of new platforms.
Therefore we see a strong motivation to turn attention back to user-oriented networks of
mobile heterogeneous devices that do not have the benefit of data center level
connectivity.

We propose to explore strong consistency mechanisms in \textit{user-centric dynamic clouds}: a
multi-user tapestry of mobile, heterogeneous devices connected via dynamic, variable-latency and
partition-prone networks.
These types of clouds are interesting because they describe a much wider range of use
cases than a data center environment, for example search and rescue clouds, mobile sensor
clouds, or even transportation clouds.
We believe that the unique challenges of this network environment require specialized
consistency models to be effective.
Because of the user-centric nature of this research, the requirement for collaboration and
sharing, and to generalize distributed storage in this type of cloud we further propose
the study of a file system as the primary data storage application.
File systems must be highly available such that a user does not notice any delay due to
coordination but must also have strong consistency such that any conflicts are presented
to the user as soon as possible.
Further, the system must be responsive when the network is unavailable or laggy, and provide strong
consistency guarantees when stable connections exist.

//=====================================================================

In the previous section, we fixed the client-centric definition of consistency by
guaranteeing that accesses should provide at least \textit{Monotonic Writes Consistency}
(MTO), which states that two updates by the same client will be serialized in the order
they arrive. Because every write maintains its parent version there is an explicit
ordering to how writes must be applied. Additionally clients are guaranteed the less
strict \textit{Read Your Writes Consistency} (RYWC) such that any accesses after a write
to version $n$ will return a version $\geq n$. These client-centric views of consistency
ensure that standard file system semantics apply at a local level, though not necessarily
globally \cite{bermbach_consistency_2013}.

//=====================================================================

reads.
The former is distinguished from the latter only if the possibility of synchronization
could have occurred.
From the point of view of the system, they are identical causes.
Forks can branch to arbitrary lengths as replicas continue to write to their latest local
copies, however when synchronization occurs a decision as to which ordering of writes is
correct must occur.

With this context and consistency model in mind, we can now identify several consistency models in order of increasing ordering strictness and define how each model's correctness criteria responds in the face of forks.
//=====================================================================

The primary form of inconsistency that we are concerned with is a fork: two
separate, conflicting writes to the same parent version.
In a homogeneous eventually consistent system implemented with anti-entropy,
the only way to minimize the number of forks is by increasing the speed of
propagation.
We have presented the best case for Eventual by implementing
\textit{bilateral} anti-entropy as well as neighbor selection likelihoods that
lead to exponential convergence of a write.
However, with a \textit{latest-writer wins} policy, the possibility of
``branch-flopping'' exists; that is after a fork occurs, the latest version is
updated on either side of the branch, and though the system will converge to a
final write, branches may be arbitrarily long and represent significantly
inconsistent states to the user.

Homogeneous Raft systems simply disallow forks to
occur by requiring that the leader determines which write is accepted and
which is not.
Because we are aggregating writes in the \texttt{AppendEntries} messages every
heartbeat interval, at most $n-1$ forks, where $n$ is the number of replicas,
are possible within the heartbeat interval.
Whichever remote write arrives at the leader first will be accepted as the
canonical branch.
Here, the branch length is bounded to the number of writes possible by a
single replica in the heartbeat interval, which is small -- and the branch is
eliminated at the heartbeat interval anyway.

The Federated system must at its core resolve how fork elimination is
communicated to the Eventual system.
A fork, by definition, is later than the earlier version.
If the Federated system simply ignores or drops forks, the Eventual cloud
might still propagate the fork around Raft.
Worse, if the Eventual portion simply drops forks without deciding on a write
to propagate, the system is no longer eventually consistent since it is
possible that given no more writes half of the replicas make one fork visible
while the other half make the other visible.

//=====================================================================

Our simulation implementation gave us the benefit of being able to manipulate
many parameters to see how replication behavior changes.
As a result, we only focused on the number of consistency messages sent.
Potentially this overstates the differences between the protocols; Raft
leaders have more processing requirements for example.
Additionally bandwidth pressure and differences in messaging might be more
important to the overall performance of a system.
However, by focusing on the number of messages and isolating version
replication from blob replication (e.g.
minimizing the amount of data required for consistent behavior), we have
allowed each protocol to fix some ``message budget''.

In truly resource constrained environments, allowing both Raft and Eventual
(either in a homogeneous or Federated context) to maintain an equal or
semi-equal message budget by modifying the $T$ parameter and allocating timing
similarly means that congestion is reduced or optimized.
Moreover, this system also allows for adaptive behavior by providing a single
place of modification and optimization; $T$ could be optimized according to a
cost function (e.g.
the message budget or other real time estimates of performance) or could be
boosted for a replica that has to do a lot of work.
This type of flexibility is important to being able to understand how minor
changes in both the environment and protocols effect overall consistency in
the system as a whole.

\pjk{Given bilateral anti-entropy and a large communication budge, nothing
beats Eventual.
In some situations Raft might be ``better'' because it's consensus, but even
w/ it we ca not implement sequential consistency.
So where is raft better?
Everyone sees the current version at the end of an epoch.
Note that raft does not prevent stale reads, but it does bound the staleness
(to the length of an epoch).
Eventual does not.
W/ Federated, we lose the ability to bound staleness tightly, so we need to
make sure that we implement out epoch-backpressure.
I'm hopeful the resulting numbers will look much better.}

% \note{Since the note above, we've implemented the forte number (backpressure)}

% \subsection{Fork Minimization}
//=====================================================================

Latency ranges for wide area networks can be extremely variable and are primarily
determined by the last mile connection.
While data centers often use backbone links to connect across the wide area, LTE networks,
satellite networks, and the cable or fiber networks that connect users introduce extra
round trip latency.
Using ICMP to measure latency across the continental United States from the Princeton
University network, Katz-Bassett et.
al report local round trip latencies in the 10-50ms range and cross-country latencies in
the 100ms range \cite{katz-bassett_towards_2006}.
However, we believe that ICMP often gets preferential routing and that the simplicity of
the echo protocol does not inform true message delays that would be experienced by a
distributed system.
We compared ICMP to a simple echo protocol implemented with GRPC~\cite{grpc} and discovered that the
ICMP latency distribution is significantly smaller than GRPC in many environments.

//=====================================================================
Our experiments use a single, very specific topology, and a natural question
arises: what are the effects of different topologies on the performance of the
system?
Consider the case where we scale the number of eventually consistent replicas
in each local area.
More eventually consistent replicas would increase the anti-entropy
propagation delay, increasing the likelihood of forks and stale reads and
decreasing the number of writes that are fully replicated.
The Raft core group could minimize the effect of this scaling to, however if
the core Raft group remained fixed, it would also increase the load on the
quorum for handling remote writes from the Eventual cloud and there would be
increasingly more drops, particularly as forks are propagated around Raft
followers.
However, the increase in the size of the Eventual cloud would also give the
system more fault tolerance as there are more paths to propagate updates when
the Raft quorum is down, particularly across the wide area.
Local outages may bring down Raft (partitioning the leader) but anti-entropy
is resilient.
