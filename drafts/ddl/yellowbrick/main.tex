\documentclass{sigkddExp}

\begin{document}

\title{Visual Steering of the Model Selection Process}

\numberofauthors{2}

\author{
\alignauthor Benjamin Bengfort and Rebecca Bilbro \\
       \affaddr{District Data Labs}\\
       \affaddr{Washington DC, USA}\\
       \email{\{bbengfort,rbilbro\}@districtdatalabs.com}
}

\additionalauthors{}

\date{06 September 2016}

\maketitle

\begin{abstract}
In this paper we introduce Yellowbrick, a Python software package that extends the popular Scikit-Learn machine learning library with \textit{visual steering} mechanisms to improve modeling performance.
\end{abstract}

\section{Introduction}

Model selection triple \cite{kumar_model_2015}.

Visualizing statistical models \cite{wickham_visualizing_2015}

Scikit-Learn \cite{pedregosa_scikit-learn:_2011} Sklearn API \cite{buitinck_api_2013}

Steering \cite{kapoor_interactive_2010}
HPC tuning and steering \cite{hollingsworth_end--end_2010,gu_falcon:_1995}

Interactive classifier builds \cite{ware_interactive_2001}, Interactive decision trees \cite{ankerst_visual_1999}

\section{Visualizers}

\subsection{Feature Analysis Visualizers}

SPLOMs \cite{wilkinson_graph-theoretic_2005}

Rank Features (1D and 2D) \cite{seo_rank-by-feature_2005}

Radviz \cite{hoffman_dna_1997,hoffman_dimensional_1999}

Parallel Coordinates \cite{fua_hierarchical_1999}

\subsection{Estimator Scoring Visualizers}

\subsubsection{Regression}

Prediction error plots and residuals plots \cite{larsen_use_1972}

\subsubsection{Most Informative Features}

Generalized linear models compute a predicted independent variable via the linear combination of an array of coefficients with an array of dependent variables. GLMs are fit by modifying the coefficients so as to minimize error and regularization techniques specify how the model modifies coefficients in relation to each other. As a result, an opportunity presents itself: larger coefficients are necessarily ``more informative'' because they contribute a greater weight to the final prediction in most cases. Additionally we may say that instance features may also be more or less ``informative'' depending on the product of the instance feature value with the feature coefficient. This creates two possibilities:

\begin{enumerate}
    \item We can compare models based on ranking of coefficients, such that a higher coefficient is ``more informative''.
    \item We can compare instances based on ranking of feature/coefficient products such that a higher product is ``more informative''.
\end{enumerate}

In both cases, because the coefficient may be negative (indicating a strong negative correlation) we must rank features by the absolute values of their coefficients. Visualizing a model or multiple models by most informative feature is usually done via bar chart where the y-axis is the feature names and the x-axis is numeric value of the coefficient such that the x-axis has both a positive and negative quadrant. The bigger the size of the bar, the more informative that feature is.

This method may also be used for instances; but generally there are very many instances relative to the number models being compared. Instead a heatmap grid is a better choice to inspect the influence of features on individual instances. Here the grid is constructed such that the x-axis represents individual features, and the y-axis represents individual instances. The color of each cell (an instance, feature pair) represents the magnitude of the product of the instance value with the feature's coefficient for a single model. Visual inspection of this diagnostic may reveal a set of instances for which one feature is more predictive than another; or other types of regions of information in the model itself.

\subsubsection{Classifiers}

Confusion Matrix (ranking models) \cite{parker_rank_2001}

ROC/AUC \cite{hanley_meaning_1982}

\subsection{Multi-Estimator Visualizers}

Grid search \cite{bergstra_algorithms_2011}

\subsection{Model Specific Visualizers}

Interactive decision trees \cite{ankerst_visual_1999}

Visualizing Naive Bayes \cite{becker_visualizing_2001}

\section{User Study}

Toward measuring visualization insight \cite{north_toward_2006}

\section{Conclusions}

\section{Acknowledgements}

\bibliographystyle{abbrv}
\bibliography{paper}

% That's all folks!
\end{document}
