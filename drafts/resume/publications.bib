
@inproceedings{bengfort_evolutionary_2014,
  title = {Evolutionary Design of Self-Organizing Particle Systems for Collective Problem Solving},
  doi = {10.1109/SIS.2014.7011790},
  abstract = {Using only simple rules for local interactions, groups of agents can form self-organizing super-organisms or flocks that show global emergent behavior. When agents are also extended with memory and goals the resulting flock not only demonstrates emergent behavior, but also collective intelligence: the ability for the group to solve problems that might be beyond the ability of the individual alone. Until now, research has focused on the improvement of particle design for global behavior; however, techniques for human-designed particles are task-specific. In this paper we will demonstrate that evolutionary computing techniques can be applied to design particles, not only to optimize the parameters for movement but also the structure of controlling finite state machines that enable collective intelligence. The evolved design not only exhibits emergent, self-organizing behavior but also significantly outperforms a human design in a specific problem domain. The strategy of the evolved design may be very different from what is intuitive to humans and perhaps reflects more accurately how nature designs systems for problem solving. Furthermore, evolutionary design of particles for collective intelligence is more flexible and able to target a wider array of problems either individually or as a whole.},
  timestamp = {2016-04-27T14:52:52Z},
  urldate = {2015-02-03},
  booktitle = {Swarm {{Intelligence}} ({{SIS}}), 2014 {{IEEE Symposium}} on},
  publisher = {{IEEE}},
  author = {Bengfort, Benjamin and Kim, Philip Y. and Harrison, Kevin and Reggia, James A.},
  month = dec,
  year = {2014},
  keywords = {Computational modeling,Evolutionary computation,Genetics,Problem-solving,Sociology,Statistics,Vectors},
  pages = {1--8},
  file = {PID3403693.pdf:/Users/benjamin/Library/Application Support/Zotero/Profiles/i8zmk51x.default/zotero/storage/PKTA7TTK/PID3403693.pdf:application/pdf;Snapshot:/Users/benjamin/Library/Application Support/Zotero/Profiles/i8zmk51x.default/zotero/storage/ZS4S7G5C/cookiedetectresponse.html:text/html}
}

@inproceedings{bengfort_efficient_2011,
  title = {Efficient Resource Allocation in {{Hybrid Wireless Networks}}},
  timestamp = {2015-08-25T16:31:56Z},
  urldate = {2015-02-03},
  booktitle = {Wireless {{Communications}} and {{Networking Conference}} ({{WCNC}}), 2011 {{IEEE}}},
  publisher = {{IEEE}},
  author = {Bengfort, Benjamin and Du, Xiaojiang},
  year = {2011},
  pages = {820--825},
  file = {PID1602635b.pdf:/Users/benjamin/Library/Application Support/Zotero/Profiles/i8zmk51x.default/zotero/storage/FUTFV3FX/PID1602635b.pdf:application/pdf;Snapshot:/Users/benjamin/Library/Application Support/Zotero/Profiles/i8zmk51x.default/zotero/storage/NZ8D298U/cookiedetectresponse.html:text/html}
}

@book{bengfort_data_2016,
  title = {Data {{Analytics}} with {{Hadoop}}: {{An Introduction}} for {{Data Scientists}}},
  isbn = {978-1-4919-1370-3},
  shorttitle = {Data {{Analytics}} with {{Hadoop}}},
  abstract = {f you're a data scientist ready to tackle statistical and machine learning techniques across large data sets, this practical guide provides a solid introduction to the world of clustered computing and analytics with Hadoop. Instead of deployment, operations, or software development, this book focuses on particular analyses you can build, the data warehousing techniques that Hadoop provides, and the higher order data workflows it can produce.

You'll learn a wide range of topics, from the basics of using MapReduce and Spark with Python to advanced modeling and data management using Spark MLlib, Hive, and HBase. You'll gain an understanding of the analytical processes and data systems available to build and empower data products that require huge amounts of data.},
  language = {English},
  timestamp = {2016-04-27T15:29:10Z},
  publisher = {{O'Reilly Media, Inc.}},
  author = {Bengfort, Benjamin and Kim, Jenny},
  month = may,
  year = {2016},
  file = {Data_Analytics_with_Hadoop.pdf:/Users/benjamin/Library/Application Support/Zotero/Profiles/i8zmk51x.default/zotero/storage/G6WGVPIA/Data_Analytics_with_Hadoop.pdf:application/pdf}
}

@misc{bengfort_teaching_2013,
  address = {New York, NY, USA},
  type = {Tutorial},
  title = {Teaching the {{Elephant}} to {{Read}}: {{Hadoop}}, {{Python}}, and {{NLP}}},
  abstract = {Many of the largest and most difficult to process data sets that we encounter tend not to be from well structured logs or databases, but rather unstructured bodies of text. In recent years, Natural Language Processing techniques have accelerated our ability to stochastically mine data from unstructured text but require large training data sets themselves to produce meaningful results. Simultaneously the growth of distributed computational architectures and file systems have allowed data scientists to deal with larger volumes of data; clearly there is common ground that can allow us to achieve spectacular results.

The two most popular open source tools for both NLP and Distributed Computing, The
Natural Language Toolkit and Apache Hadoop, are written in different languages Python and Java. We will discusses the methodology to integrate them using Hadoop's Streaming interface which sends and receives data into and from mapper and reducer scripts via the standard file descriptors.},
  timestamp = {2015-03-22T18:38:06Z},
  author = {Bengfort, Benjamin and Murphy, Sean Patrick},
  month = oct,
  year = {2013}
}

@book{ojeda_practical_2014,
  title = {Practical {{Data Science Cookbook}}},
  isbn = {978-1-78398-024-6},
  abstract = {As increasing amounts of data is generated each year, the need to analyze and operationalize it is more important than ever. Companies that know what to do with their data will have a competitive advantage over companies that don't, and this will drive a higher demand for knowledgeable and competent data professionals.

Starting with the basics, this book will cover how to set up your numerical programming environment, introduce you to the data science pipeline (an iterative process by which data science projects are completed), and guide you through several data projects in a step-by-step format. By sequentially working through the steps in each chapter, you will quickly familiarize yourself with the process and learn how to apply it to a variety of situations with examples in the two most popular programming languages for data analysis\textemdash{}R and Python.},
  language = {English},
  timestamp = {2016-04-27T14:57:40Z},
  urldate = {2015-05-11},
  publisher = {{Packt Publishing Ltd}},
  author = {Ojeda, Tony and Murphy, Sean Patrick and Bengfort, Benjamin and Dasgupta, Abhijit},
  month = sep,
  year = {2014},
  file = {[PDF] from francischan.com:/Users/benjamin/Library/Application Support/Zotero/Profiles/i8zmk51x.default/zotero/storage/G2DT9QA6/Ojeda et al. - 2014 - Practical Data Science Cookbook.pdf:application/pdf;Snapshot:/Users/benjamin/Library/Application Support/Zotero/Profiles/i8zmk51x.default/zotero/storage/42FC6A85/books.html:text/html;Snapshot:/Users/benjamin/Library/Application Support/Zotero/Profiles/i8zmk51x.default/zotero/storage/PP283WSF/books.html:text/html}
}

@inproceedings{bengfort_interactive_2015,
  title = {Interactive {{Knowledge}}-{{Goal Reasoning}}},
  timestamp = {2015-08-25T16:32:46Z},
  urldate = {2015-08-25},
  booktitle = {Goal {{Reasoning}}: {{Papers}} from the {{ACS Workshop}}},
  author = {Bengfort, Benjamin and Cox, Michael T.},
  year = {2015},
  pages = {10},
  file = {[PDF] from gatech.edu:/Users/benjamin/Library/Application Support/Zotero/Profiles/i8zmk51x.default/zotero/storage/564K5HI4/Bengfort et al. - 2015 - Interactive Knowledge-Goal Reasoning.pdf:application/pdf}
}

@misc{bengfort_natural_2016,
  address = {Portland, Oregon},
  type = {Tutorial},
  title = {Natural {{Language Processing}} with {{NLTK}} and {{Gensim}}},
  abstract = {Natural Language Processing (NLP) is often taught at the academic level from the perspective of computational linguists. However, as data scientists, we have a richer view of the natural language world - unstructured data that by its very nature has latent information that is important to humans. NLP practitioners have benefited from machine learning techniques to unlock meaning from large corpora, and in this class we'll explore how to do that particularly with Python, Gensim, and the Natural Language Toolkit (NLTK).

NLTK is an excellent library for machine-learning based NLP, written in Python by experts from both academia and industry. Python allows you to create rich data applications rapidly, iterating on hypotheses. The combination of Python + NLTK means that you can easily add language-aware data products to your larger analytical workflows and applications.

In this tutorial we will begin by exploring NLTK from the view of the corpora that it already comes with, and in this way we will get a feel for the various features and functionality that NLTK has. However, most NLP practitioners want to work on their own corpora, therefore during the second half of the tutorial we will focus on building a language aware data product from a specific corpus - a topic identification and document clustering algorithm from a web crawl of blog sites. The clustering algorithm will use a simple Lesk K-Means clustering to start, and then will improve with an LDA analysis using the Gensim library.},
  language = {English},
  timestamp = {2016-07-19T14:58:18Z},
  author = {Bengfort, Benjamin},
  collaborator = {Lorenz, Laura and Bilbro, Rebecca and Voorhees, Will},
  month = may,
  year = {2016},
  file = {Natural Language Processing with NLTK and Gensim_ District Data Labs Tutorial at PyCon 2016.pdf:/Users/benjamin/Library/Application Support/Zotero/Profiles/i8zmk51x.default/zotero/storage/AWAURS9I/Natural Language Processing with NLTK and Gensim_ District Data Labs Tutorial at PyCon 2016.pdf:application/pdf}
}

@misc{bengfort_visualizing_2016,
  address = {Seattle, Washington},
  type = {Talk},
  title = {Visualizing the {{Model Selection Process}}},
  abstract = {Machine learning is the hacker art of describing the features of instances that we want to make predictions about, then fitting the data that describes those instances to a model form. Applied machine learning has come a long way from it's beginnings in academia, and with tools like Scikit-Learn, it's easier than ever to generate operational models for a wide variety of applications. Thanks to the ease and variety of the tools in Scikit-Learn, the primary job of the data scientist is \_model selection\_. Model selection involves performing feature engineering, hyperparameter tuning, and algorithm selection. These dimensions of machine learning often lead computer scientists towards automatic model selection via optimization (maximization) of a model's evaluation metric. However, the search space is large, and grid search approaches to machine learning can easily lead to failure and frustration. Human intuition is still essential to machine learning, and visual analysis in concert with automatic methods can allow data scientists to steer model selection towards better fitted models, faster. In this talk, we will discuss interactive visual methods for better understanding, steering, and tuning machine learning models.},
  timestamp = {2016-04-27T15:03:44Z},
  author = {Bengfort, Benjamin},
  month = jul,
  year = {2016}
}

@book{bengfort_applied_2017-1,
  title = {Applied {{Text Analysis}} with {{Python}}: {{Enabling Language Aware Data Products}} with {{Machine Learning}}},
  isbn = {978-1-4919-6304-3},
  shorttitle = {Applied {{Text Analysis}} with {{Python}}},
  abstract = {The programming landscape of natural language processing has changed dramatically in the past few years. Machine learning approaches now require mature tools like Python's scikit-learn to apply models to text at scale. This practical guide...},
  language = {English},
  timestamp = {2017-01-04T15:35:13Z},
  urldate = {2017-01-04},
  publisher = {{O'Reilly Media, Inc.}},
  author = {Bengfort, Benjamin and Bilbro, Rebecca and Ojeda, Tony},
  month = aug,
  year = {2017},
  file = {Snapshot:/Users/benjamin/Library/Application Support/Zotero/Profiles/i8zmk51x.default/zotero/storage/A8GH8CDM/0636920052555.html:text/html}
}

@misc{bengfort_data_2016-2,
  type = {Webinar},
  title = {Data {{Product Architectures}}},
  abstract = {Data products derive their value from data and generate new data in return. As a result, machine-learning techniques must be applied to their architecture and development. Machine learning fits models to make predictions on unknown inputs and must be...},
  language = {English},
  timestamp = {2017-01-04T15:36:55Z},
  urldate = {2017-01-04},
  author = {Bengfort, Benjamin},
  month = dec,
  year = {2016},
  file = {Snapshot:/Users/benjamin/Library/Application Support/Zotero/Profiles/i8zmk51x.default/zotero/storage/2QHXXPV3/3800.html:text/html}
}

@misc{bengfort_dynamics_2016-1,
  address = {Raleigh, NC},
  title = {Dynamics in {{Graph Analysis}}: {{Adding Time}} as a {{Structure}} for {{Visual}} and {{Statistical Insight}}},
  shorttitle = {Dynamics in {{Graph Analysis}}},
  abstract = {Modeling data as networks of relationships between entities can be a powerful method for both visual analytics and machine learning; people are very good at distinguishing patterns from interconnected structures, and machine learning methods get a performance improvement when applied to graph data structures. However, as these structures become more complex or embed more information over time, both visual and algorithmic methods get messy; visual analyses suffer from the "hairball" effect, and graph algorithms require either more traversal or increased computation at each vertex. A growing area to reduce this complexity and optimize analytics is the use of interactive and subgraph techniques that model how graph structures change over time.

In this talk, I demonstrate two practical techniques for embedding time into graphs, not as computational properties, but rather as structural elements. The first technique is to add time as a node to the graph, which allows the graph to remain static and complete, but minimizes traversals and allows filtering. The second is to represent a single graph as multiple subgraphs where each is a snapshot at a particular time. This allows us to use time series analytics on our graphs, but perhaps more importantly, to use animation or interactive methodologies to visually explore those changes and provide meaningful dynamics.},
  timestamp = {2017-01-04T16:03:49Z},
  urldate = {2017-01-04},
  author = {Bengfort, Benjamin},
  month = sep,
  year = {2016},
  file = {PyData Carolinas 2016 | Presentation\: Dynamics in Graph Analysis\: Adding Time as a Structure for Visual and Statistical Insight:/Users/benjamin/Library/Application Support/Zotero/Profiles/i8zmk51x.default/zotero/storage/6Q5Q6EI9/39.html:text/html}
}
