\documentclass[10pt,twocolumn]{article}

\usepackage{pete}
\usepackage{times}
\usepackage{fullpage}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{xspace}
\usepackage{amsmath}

\begin{document}

\title{\bf Scaling Strong Consistency Across Continents\\
with Hierarchical Consensus}
% \author{\rm{Benjamin Bengfort} and \rm{Pete Keleher}\\
% University of Maryland, College Park\\
% \{bengfort,keleher\}@cs.umd.edu}
\author{\emph{Omitted for review}}
\date{}

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}

\begin{abstract}
    \input{abstract}
\end{abstract}

\section*{Introduction}

Distributed consensus algorithms are \fuzzy{the primary mechanism} that
coordinate independent machines inside a computing cluster.
% emphasis on independent
Coordination is essential to large-scale software systems, particularly
modern systems that span globe~\cite{spanner,}, therefore the demanding requirements for
consensus include high throughput, low latency, fault tolerance, durability,
and consistent replication.



Consensus design has traditionally considered progress in the face of one or
two node failures.
As a result, current
approaches~\cite{mencius,epaxos,multicoordinated_paxos,spaxos,mdcc}
assume a small number of replicas in a co-located consensus group, each
centrally located on a powerful and reliable host.

Many consensus algorithms have their roots in the Paxos~\cite{paxos}

Move from a single leader to a democratic system. Now to representative government. 

In this paper we tell the story of HC, a framework to create consensus across
hundreds of replicas making decisions across an object space that is not
independent and therefore requires strong consistency.
We note that like in human societies, democracy works well in small groups
but does not scale well to large groups, even for binary decisions (the e.g.
the kind of decisions that distributed systems make).
Inspired by modern governments, we propose a representative system of
consensus, such that replicas elect leaders to participate in a \roo that
makes decisions about the global state of the system.
Local decision making, the kind that effects only a subset of clients and
objects is handled locally by \subs as efficiently as possible.

As always, the devil is in the details.
In this paper we first walk through the consistency model, then discuss the
components of the system in an understandable way by using Raft as our base
consensus algorithm and a key/value store as our primary application.
Most of the rest of a paper is a discussion of safety.
How can things go wrong in such a system? What are the implications? How can
the system adapt, remain resilient, and even outperform other fault tolerant
distributed consensus?
We conclude with an evaluation and a discussion of related works ...

\strong{problem}:

\begin{itemize}
    \item standard consensus doesn't scale well
    \item it gets even worse across the wide area
    \item current systems assume object-independence
    \item current systems use fixed topographies and do not adapt
\end{itemize}

\strong{motivation}:

\begin{itemize}
    \item hierarchies already exist in applications
    \item geo-replication requires adaptivity
    \item transactions don't work well with tablet structures \pjk{We
        don't discuss them in this paper.  Do we?}
\end{itemize}

\strong{claims}:

\begin{itemize}
    \item HC is an implementation and extension of Vertical Paxos~\cite{vertical_paxos} that organizes consensus hierarchically.
    \item Use Raft~\cite{raft} for understandability
    \item No requirement for SQ protocol to be Raft, however there must be \roo and \sub intersections.
    \item we assert that leader-oriented protocols work best.
    \item generalized framework for scaling consensus
\end{itemize}

\section*{Background}

Paxos~\cite{fast_paxos,camargos_multicoordinated_2007,lamport_generalized_2005,epaxos,spaxos}

We implement and extend Vertical Paxos~\cite{vertical_paxos} but go beyond
primary backup replication to creating a key/value store (and soon a file
system).

Consider Niobe~\cite{niobe} and Boxwood~\cite{boxwood} as two implementations
of Vertical Paxos.

\subsection*{Raft Consensus}

A lightweight description of Raft and leader-oriented consensus protocols.

\subsection*{Vertical Consistency Model}

A command is identified by \texttt{(Epoch, Term, Log Index)} -- mirrors
Vertical Paxos \texttt{(Configuration, Ballot, Index)} structure.

Consistency: there must be a single, externalized ordering across all
objects.

Figure: grid ordering consistency.  \pjk{??}

\subsection*{Related Work}

\begin{itemize}
    \item ePaxos~\cite{epaxos} is best geo-replicated consensus, doesn't
    scale.
    \item MDCC~\cite{mdcc}, Spanner~\cite{spanner} are big systems across
    multiple data centers but isolate objects into tablets that aren't
    coordinated. \pjk{In what sense?  Does MDCC not re-configure tablets?}
    \item Niobe~\cite{niobe} and Boxwood~\cite{boxwood} are Vertical Paxos
    for primary replication but treat configuration as an independent master
    quorum and have independent object management.
\end{itemize}

\subsection*{HC Goals/Claims/Intuitions}

\strong{consistency claims:} We claim that for a current epoch, $E_i$,
this externalized ordering\footnote{\pjk{Hmm.. Externalized implies
    linearizability. Accesses will observe this, but that does not
    mean that we can rebuild it later on.  This comment seems to be
    more true for SC.}} exists
for all epochs $\leq E_{i-j}$ such that $j\geq1$ and $E_{i-j}$ is
\emph{closed}, e.g. a tombstone has been written to the logs of all \subs,
$S_{i-j}$.
In the current epoch, $E_i$ it is not possible to externalize a complete
ordering,  however it is possible to describe an observable sequential
ordering of all objects.

\strong{performance claims:} we should make some.

\section*{Design}\footnote{\pjk{I'd make this a straw-man design, along the
  lines of SUNDR. We can then add delegation and generalized
  delegation later.}}

Preliminaries: a system is composed of a set of replicas, $R$, initially we
assume that this set is fixed, later we discuss adding or removing to this
set.
The system self organizes into a \roo.
The \roo assigns replicas to zero or one \subs.

During operation, a Replica can be \emph{one} of the following.

\begin{enumerate}
    \item A follower in a \sub
    \item A leader of a \sub and a member of the \roo
    \item A hot spare
\end{enumerate}

Replicas that are leaders can simultaneously be a \sub leader and a \roo
leader.
Later we relax this so that any replica can be a member of of the \roo.

\subsection*{Elections and Delegation}

\begin{itemize}
    \item election of \roo leader
    \item assignment/election of \sub leaders
    \item delegation of votes
    \item re-elections
    \item delegations expire
\end{itemize}

\subsection*{Data Model}

\ben{maybe move this to a different location?}

\begin{itemize}
    \item key/value store
    \item read then write updates \pjk{We assume all writes
        implemented w/ read then write, or do we implement writes this
        way? And to what purpose?}
    \item accesses: get, put, delete
\end{itemize}

\subsection*{Operation/Decision Making}

\begin{itemize}
    \item \sub decision making
    \item remote reads and writes
    \item reconfiguration/epoch changes
    \item \roo decision making
    \item transitions/fuzzy epochs
\end{itemize}

\section*{Safety and Fault Tolerance}

\begin{itemize}
    \item individual faults
    \item partitions of the network (big faults)
    \item safety argument
\end{itemize}

\section*{Implementation and Evaluation}

\begin{itemize}
    \item Go implementation
    \item AWS EC2 across 15 regions and 150 replicas (3, size 3 subquorums in 15 regions with 15 hot spares).
    \item LevelDB
    \item Version numbers
\end{itemize}

Evaluation

\section*{Conclusion}

\begin{itemize}
    \item \todo{Psuedo code to make paper clearer}
    \item \bobby{Provide top down description, where each model is relatively small, reduce cognitive load}
    \item \bobby{Change name ``nuclear option'' -- make more technical e.g. ``final recovery'' (too jocular)}
\end{itemize}

\bibliography{bib}
\bibliographystyle{abbrv}

\end{document}
