\documentclass[11pt,conference]{IEEEtran}

% IEEE standard packages
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

% Extra packages
\usepackage{booktabs} % For formal tables
\usepackage{xspace}
\usepackage{pete} % For Pete's annotations

% Writing helpers
\newcommand{\hc}{hierarchical consensus\xspace}
\newcommand{\Hc}{Hierarchical consensus\xspace}
\newcommand{\sub}{subquorum\xspace}
\newcommand{\Sub}{Subquorum\xspace}
\newcommand{\subs}{subquorums\xspace}
\newcommand{\Subs}{Subquorums\xspace}
\newcommand{\sys}{Alia\xspace}
\newcommand{\roo}{root quorum\xspace}
\newcommand{\roos}{root quorums\xspace}
\newcommand{\Roo}{Root quorum\xspace}
\newcommand{\Roos}{Root quorums\xspace}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Consensus Across Continents}

\author{
\IEEEauthorblockN{Benjamin Bengfort, Rebecca Bilbro, Pete Keleher}
\IEEEauthorblockA{Department of Computer Science\\
University of Maryland, College Park, MD, USA\\
\{bengfort,rbilbro,keleher\}@cs.umd.edu}}

\maketitle

\begin{abstract}
    \input{abstract}
\end{abstract}

\begin{IEEEkeywords}
hierarchical consensus, geographic replication, delegated voting, strong consistency
\end{IEEEkeywords}

\section{Introduction}
% The intro seems like a very good narrative and captures all of the moving pieces
% However, I'm worried that we're not getting across our awesome sauce soon enough (e.g. the first page)
% I'm also worried that we do not do enough to describe our system in a meaningful way
% Other papers like Akkio have 2 page intros though ...
% Just not sure how to do this narrative and lift things to the top
% To help this, I've added the system architecture diagram (fig 1) to the second page
% and referenced it, even though it's not considerded in detail until later in the paper -
% not totally sure if this is a good idea or not.

The recent availability of cloud service providers with data centers that span the globe
has made it easier than ever before to deploy geographically distributed data systems
that span continents and oceans.
These types of systems increase local performance by minimizing network distance between
users and replicas and provide the opportunity for data recovery in the face of
catastrophes such as floods or earthquakes.
Moreover, the success of specialized, high-availability data
systems~\cite{megastore,tao,akkio,dynamic_placement} in maximizing throughput across the
wide area has led to increased interest in geo-replicated systems, particularly as more
applications are being developed with international audiences in mind.
However, in order to generalize these systems for modern application development, strong
consistency semantics is required; therefore managed replicated data
services~\cite{spanner,aurora,cockroachdb} have risen to prominence, achieving these
semantics by hiding both the replication and infrastructure complexity from developers.

At the same time, traditional monolithc applications are being replaced by microservice
architectures and cloud-native service meshes~\cite{envoy} that make
infrastructure directly visible to applications.
As applications scale, service meshes make it easier to maintain and optimize
service-specific communication to minimize downtime and to improve system flexibility.
Additionally, due to increasing privacy regulation, application developers require more
control over data placement rather than less~\cite{gdpr}.
The engineering-based solutions of managed geo-distributed data services are designed to
coordinate hundreds of replicas that have access to expensive data-center hardware and
involves multiple, independent processes and quorums to synchronize time, allocate locks,
manage transations, and recover from failure.
Although these systems provide strong consistency, they do so in an rigid, opaque manner
that is not flexible enough for developers who require strong consitency at a higher
level of the application stack.

We propose a simpler approach to building large, geographically replicated systems.
Rather than relying on a fleet of loosely-coupled, independent small quorums whose
interactions are difficult to reason about, we propose a single, system-wide consensus
protocol that coordinates both replica placement and data accesses.
By ensuring that all coordination occurs through a single consensus activity,
it is easier to reason about the consistency of the system even in a network environment
prone to correlated failures, partitions, and variable latency.
Additionally, a single source of coordination gives the system the freedom to adapt to
changes in access patterns, configure to maximize throughput, specify data placement
rules, and ensure straightforward system maintenance.

In order to achieve this, a new consensus protocol that can scale beyond a handful of
replicas is required.
Distributed consensus, canonically represented by Paxos~\cite{paxos_simple} and its
performance optimizing
variants~\cite{fast_paxos,multicoordinated_paxos,spaxos,generalized_paxos}, primarily
consider safety in the case of one or two fail-stop node failures.
Although some recent research has explored the problem of geo-distributed
consensus~\cite{mencius,epaxos}, it primarily considers the problem of high-latency
links but geo-replication implies scale.
Services running around the globe recquire dozens if not hundreds of replicas and
introduce new failure modes such as network partitions, where sections of the system
operate independently without fail-stop failure, and highly variable latency that
inhibit quorum progress.
In order to scale systems beyond a handful of replicas, current
systems~\cite{spanner,scatter,mdcc,calvinfs} use Paxos as a component, instantiated
across multiple transactions, shards, or tablets to manage small subsystems
independently, leading to increased complexity and reduced transparency.

% the below paragraph needs help
We introduce a novel approach to scale consensus beyond a handful of nodes:
\emph{\hc}.
Our approach is to similarly decompose the consensus problem into units that can be
handled by provenly safe algorithms, but organizes all managed processes into an
intersecting hierarchy of quorums that ensure that all system-wide consensus decisions
are totally ordered.
The challenge is in building a multi-group coordination protocol that configures and
mediates \subs through a \roo.
The \roo gaurantees correctness by pivoting the system through reconfigurations that
place replicas into \subs and maps them to partitions of the object namespace to handle
direct data accesses.

\ben{ended here}

% Contributions:
% - Reconfigurable consensus + adaptability
% - Delegated voting to scale consensus
% - Fuzzy transitions to allow progress
% - Access/policy optimized data placement rules


The \roo is composed of all replicas in the system, although reconfigurations are rare
with respect to data accesses, we introduce \emph{delegated voting} to optimize quorum
decisions at the root.

Much of the systems complexity comes from handshaking between the \roo and \subs during
reconfiguration.
These handshakes are made easier and far more efficient by using \emph{fuzzy transitions},
which allow individual \subs to move through reconfiguration at their own pace without

We validate our approach by implementing \hc in Alia, a linearizable object store
explicitly intended to run with many replicas, geo-replicated across heterogenous
networks and devices.
The resulting system is local, in that replicas serving clients can be located near them.
The system is fast because individual operations are served by a small group of replicas
regardless of the size of the total system.
The system is nimble in that it it can dynamically reconfigure the number, membership,
and responsibilities of the subquorums in response to failures, phase changes in the
driving applications or policy requirements for data placement and durability.
Finally, the system is consistent, supporting the strongest form of per-object
consistency without relying on special-purpose hardware.
We demonstrate its advantages through an implementation scaling to hundreds of replicas
across more than a dozen availability zones around the world using Amazon EC2.

% TODO: do we need rest of paper outline or contributions here?


\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/election3}
    \caption{Replicas participate in intersecting tiers of consensus.}
    \label{fig:system}
\end{figure}


\section{Conclusion}



Future work: investigate more tiers

\bibliographystyle{plain}
\bibliography{papers}

\end{document}