\documentclass[10pt,conference]{IEEEtran}

% IEEE standard packages
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

% Extra packages
\usepackage{booktabs} % For formal tables
\usepackage{xspace}
\usepackage{pete} % For Pete's annotations

% Writing helpers
\newcommand{\hc}{hierarchical consensus\xspace}
\newcommand{\Hc}{Hierarchical consensus\xspace}
\newcommand{\sub}{subquorum\xspace}
\newcommand{\Sub}{Subquorum\xspace}
\newcommand{\subs}{subquorums\xspace}
\newcommand{\Subs}{Subquorums\xspace}
\newcommand{\sys}{Alia\xspace}
\newcommand{\roo}{root quorum\xspace}
\newcommand{\roos}{root quorums\xspace}
\newcommand{\Roo}{Root quorum\xspace}
\newcommand{\Roos}{Root quorums\xspace}

\renewcommand{\pjk}[1]{{\bf
    [\marginpar[\hbox{{\textcolor{blue}{pjk}}\raisebox{0ex}{\Huge $\rightarrow$}}]%
{\hbox{\raisebox{0ex}{\Huge $\leftarrow$}{\textcolor{blue}{pjk}}}}\textcolor{blue}{#1}]}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Consensus Across Continents}

\author{
\IEEEauthorblockN{Benjamin Bengfort, Rebecca Bilbro, Pete Keleher}
\IEEEauthorblockA{Department of Computer Science\\
University of Maryland, College Park, MD, USA\\
\{bengfort,rbilbro,keleher\}@cs.umd.edu}}

\maketitle

\begin{abstract}
    \input{abstract}
\end{abstract}

\begin{IEEEkeywords}
hierarchical consensus, geographic replication, delegated voting, strong consistency
\end{IEEEkeywords}

% NOTES:

% - tags and tagspace is out, objects and namespace partition is in
% - there are a lot of places to expand, but we are space limited for ICDCS

\section{Introduction}
\label{section:intro}

Geographically distributed data systems now commonly span continents and oceans.
These systems leverage data centers newly available around the globe,
increasing local performance by minimizing network distance between users and
replicas, and offering data recovery in the face of catastrophes such as floods or
earthquakes.
Specialized, high-availability data systems~\cite{megastore,tao,akkio,dynamic_placement}
have maximized throughput across the wide area, driving interest in
geo-replicated systems and making truly international applications increasingly
feasible.
To generalize geographically distributed systems, managed replicated data
services~\cite{spanner,aurora,cockroachdb} that can provide strong consistency semantics
have risen to prominence.
However, the solutions introduced by these new systems and services require specialized
hardware and engineering involving multiple independent subsystems with different
failure modes, which, while providing strong consistency to application developers, do
so by hiding both replication and infrastructure complexity.

Unfortunately, this complexity is increasingly necessary for modern application
development.
Traditional monolithic applications are being replaced by microservice architectures
and cloud-native service meshes~\cite{envoy} that make infrastructure directly visible
to applications.
Developers in turn leverage this visibility to scale applications, for instance using
service meshes to maintain and optimize service-specific communication, minimize
downtime, localize data to users, and improve system flexibility.
This visibility is also critical to meeting the demands of increasing privacy
regulations, which require applications developers to finely control data
placement based upon differing legal obligations of the locales of users~\cite{gdpr}.
Thus, while managed data services provide strong consistency, their rigidity and
opaqueness are not flexible enough for developers who require strong consistency
at a higher level of the application stack.

This paper presents a new protocol, \emph{\hc}, motivated by the need for a simpler
and more general-purpose approach to building large, geographically replicated
systems.
The engineering-based solutions of managed geo-distributed data services rely on
multiple, independent microservices and quorums together with expensive data-center
hardware to synchronize time, allocate locks, manage transactions, and recover from
failure.
We instead propose a \emph{single, system-wide consensus protocol} that coordinates both
replica placement and data accesses.
By ensuring that all coordination occurs through a single consensus activity rather
than a fleet of small, independent quorums, it is easier to reason about the
consistency of the system even in a network environment prone to correlated failures,
partitions, and variable latency.
This single source of coordination then frees the system to adapt to changes in access
patterns, configure to maximize throughput, specify data placement rules, and ensure
straightforward system maintenance.

In order to achieve this, a new consensus protocol that can scale beyond a handful of
replicas is required.
Distributed consensus, canonically represented by Paxos~\cite{paxos_simple} and its
performance optimizing
variants~\cite{fast_paxos,multicoordinated_paxos,spaxos,generalized_paxos}, primarily
considers safety in the case of one or two fail-stop node failures.
Recent research has explored the problem of geo-distributed
consensus~\cite{mencius,epaxos}, but primarily considers the problem of high-latency
links.
% We imply that the above algorithms do not scale, we may need to explicitly state and
% cite flexible paxos or our own research in \ref{fig:scaling_consensus}.
However, geo-replication implies scale.
Services running around the globe require dozens if not hundreds of replicas and
introduce new failure modes such as network partitions, where sections of the system
operate independently without fail-stop failure, and highly variable latency that
inhibit quorum progress.
% the next sentence seems a little repetitious - revise for camera ready
To scale systems beyond a handful of replicas, current
systems~\cite{spanner,scatter,mdcc,calvinfs} use Paxos as a component, instantiated
across multiple transactions, shards, or tablets to manage small subsystems
independently, making it difficult to reason about consistency.

% the below paragraph needs help
\Hc introduces a novel approach to scale consensus beyond a handful of nodes, wherein
the consensus problem is effectively decomposed into process units.
This is achieved with a multi-group coordination protocol that configures and
mediates \subs through a \roo.
The \roo guarantees correctness by pivoting the system through \emph{reconfigurations}
that place replicas into \subs, mapping them to partitions of the object namespace.
Each \sub serializes accesses to its mapped objects using provenly safe algorithms,
placed to maximize throughput or durability.
To ensure that all system-wide consensus decisions are totally ordered with respect to
changes in reconfiguration, all \subs intersect the \roo in a hierarchy of quorums.

The \roo is composed of all replicas in the system.
Though decisions made by the \roo are rare with respect to data accesses, we introduce
\emph{delegated voting} to optimize quorum decisions at the root.
Much of the system's complexity comes from handshaking between the \roo and \subs during
reconfiguration.
These handshakes are made easier and far more efficient by using \emph{fuzzy transitions},
which allow individual \subs to move through reconfiguration at their own pace without
impeding progress.
Finally, subquorum consensus can be optimized for policy-driven \emph{data placement},
allowing objects that require more throughput to use leader-oriented consensus whereas
objects that require stronger durability can be replicated across data centers using
optimistic fast-path consensus.

We validate our approach by implementing \hc in Alia, a linearizable object store
explicitly intended to run with many replicas, geo-replicated across heterogenous
networks and devices.
The resulting system is local, in that replicas serving clients can be located near them.
The system is fast because individual operations are served by a small group of replicas
regardless of the size of the total system.
The system is nimble in that it it can dynamically reconfigure the number, membership,
and responsibilities of the subquorums in response to failures, phase changes in the
driving applications or policy requirements for data placement and durability.
Finally, the system is consistent, supporting the strongest form of per-object
consistency without relying on special-purpose hardware.
We demonstrate its advantages through an implementation scaling to hundreds of replicas
across more than a dozen availability zones around the world using Amazon EC2.

% TODO: do we need rest of paper outline or contributions here?
% NOTE: we've left out the paper outline and specific contributions due to space

% Contributions:
% - Reconfigurable consensus + adaptability
% - Delegated voting to scale consensus
% - Fuzzy transitions to allow progress
% - Access/policy optimized data placement rules

\section{Background}
\label{section:background}
% This is a very short initial background section, mostly because of space constraints.
% With a longer page limit, I recommend expanding this section a bit

Distributed consensus algorithms ensure that multiple replicas maintain an identical
state by applying commands in the same order, ensuring a consistent, externalizable
view of the system when any replica is queried.
Most of these algorithms are based on Paxos~\cite{paxos_simple}, which ensures that a
distributed log of ordered commands is maintained even if $f$ replicas fail-stop.
This is achieved with quorums of $2f+1$ replicas that write an entry into the log using
a 2 phase balloting process: \texttt{PREPARE} and \texttt{ACCEPT}.
The \texttt{PREPARE} phase is designed to nominate an open slot in the log at position
$i$ to place the command and to detect any conflicting commands that may already
exist in that slot; the \texttt{ACCEPT} phase commits the command in that slot.
In both phases a majority of replicas must respond for progress to be made.

There are two primary optimizations to the basic consensus algorithm: leader election
and fast path execution.
Leader-oriented consensus protocols elide the \texttt{PREPARE} phase by specifically
selecting a replica that has sole responsibility of slot nomination.
Some variants such as Raft~\cite{raft} elect a leader that can nominate all slots for
a time-limited duration, where as others like Mencius~\cite{mencius} use a round-robin
approach to assigning leadership.
Fast path execution such as Fast Paxos~\cite{fast_paxos} and EPaxos~\cite{epaxos}
optimistically push through commits on the \texttt{PREPARE} phase and use conflict
detection mechanisms to determine if a slow path \texttt{ACCEPT} is required.
Both of these optimizations are designed to reduce communication rounds in the common
case, while still maintaining safety when $f$ replicas fail, however none of these
optimizations describe how to scale consensus beyond $2f+1$ replicas.

Vertical Paxos~\cite{vertical_paxos,niobe} describes how to scale consensus groups by
allowing an auxiliary master quorum to execute safe reconfiguration directives in the
middle of consensus decisions.
The replicated state machine process is extended from a single log to a grid where
commands are placed both horizontally as a sequence of consensus instances
(configurations) and vertically as increasing ballot numbers (log indices).
Because consensus instances are disjoint inside of a single configuration, Vertical
Paxos creates a total ordering of commands that is guaranteed to bring the system to
an identical state no matter the point in execution.
By allowing multiple, active consensus instances, Vertical Paxos decouples command
execution and state transfer from reconfiguration, allowing the system to arbitrarily
scale.
While Vertical Paxos considers scaling consensus and other algorithms such as EPaxos and
Mencius consider geo-distributed consensus, no consensus protocol tackles scaling
consensus to hundreds of nodes across the wide area.

\section{Hierarchical Consensus}
\label{section:hc}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/election3}
    \caption{\Hc is an extension of Vertical Paxos wherein the \roo moves the system
    through reconfigurations, and intersecting \subs manage local data accesses.}
    \label{fig:system}
\end{figure}

\Hc and Alia are an extension and implementation of Vertical Paxos that are specifically
designed for scalable, geo-distributed consensus.
Like Vertical Paxos, \hc (HC) organizes replicas into two tiers of quorums, each
responsible for fundamentally different decisions, as shown in Figure \ref{fig:system}.
The lower tier consists of multiple independent subquorums, each committing data access
operations to local shared logs.
The upper, root quorum, consists of subquorum peers, usually their leaders, delegated to
represent the subquorum and hot spares in root elections and reconfiguration commits.
\Hc's main function is to export a linearizable abstraction of shared accesses to some
underlying substrate, such as a distributed object store or file system.
We assume that nodes hosting object stores, applications, and HC are frequently
co-located across the wide area and introduce our object store implementation, Alia,
in Section~\ref{section:implementation}.
% Should we use \textsection instead of "Section" for cross references?
% The intro section should be expanded with system assumptions, e.g.

The \roo's primary responsibility is to manage the configuration of $p$ distinct
processes, each of which maintain their own local state and can handle access requests
from users to objects represented by a namespace, $N$.
It does this by mapping processes to \subs and mapping \subs to partitions of the
namespace, allocating all extra processes as \emph{hot spares}, which stand by to be
placed in a \sub on individual process failure.
Each distinct mapping of the namespace to replicas is a single configuration and is
represented by an \emph{epoch}, $e$, a monotonically increasing index that describes
the horizontal position of the Vertical Paxos grid, and is used by the system to
guarantee safety.
The set of \subs specified by each epoch is represented by $Q_e$ such that an individual
\sub is identified as $q_{i,e}$.
% In this paper, we describe the system as $p/\|q\|$ where $\|q\|$ is the expected size of
% each \sub, usually $2f+1$ where $f$ is the tolerated failure of any individual \sub, e.g. a
% $30/5$ system has 25 processes that can be mapped to up to 6 \subs with 5 replicas each,
% tolerating $f=2$ failures per \sub.

The \roo partitions (shards) the namespace across $Q_e$ by mapping a subset of the
namespace across the \subs, $n_i \mapsto q_{i,e}$, ensuring that each shard of the
namespace is a disjoint subset such that
$\forall n_i \in N_e$ $\exists! q_{i,e} \mapsto n_i$.
The intent of subquorum localization is ensure that the \emph{domain} of a client, the
portion of the namespace it accesses, is entirely within the scope of a local, or
nearby, subquorum.
To the extent that this is true across the entire system, each client interacts with
only one subquorum, and subquorums do not interact at all during execution of a single
epoch.
This \emph{siloing} of client accesses simplifies implementation of strong consistency
guarantees and allows better performance at the cost of restricting multi-object
transactions.
% This last sentence is kind of weird
We use agility to attempt to get this, but allow client-side multi-object transactions.

% This section is supposed to serve as an outline of the section: reconfiguration,
% then fuzzy transitions, then placement, then delegated voting - but doesn't do it in
% the order of the section or use section references ... then tries to segue.
The \roo is a consensus group consisting of delegates (usually subquorum leaders) whose
primary consensus operation is to commit new epochs.
As \subs transition to new epochs, they handoff their current state to the \sub(s)
newly responsible for managing that portion of the namespace, ensuring that no portion
of the namespace can be accessed concurrently in two different epochs.
For understandability we describe the \hc in leader-oriented terms, though there is no
requirement for either the \roo or the \subs to implement leader-oriented consensus.
Further, we propose that \subs should be assigned the consensus algorithm that best
optimizes for the durability and throughput requirements of the shards they manage.
While the root quorum is composed of all replicas in the system, only a subset of
replicas actively participates in root quorum decision making, in the common case.
Using these mechanisms, \hc prioritizes flexibility and transparency, ensuring the
system is safe while able to make progress and can be managed easily while scaled to
very large consensus groups across continents and oceans.

\subsection{Delegated Voting}
\label{section:delegation}

From a logical perspective, the \roo's membership is the set of all system replicas, at
all times.
This ensures that all \subs intersect with the \roo such that all commands are totally
ordered, and to improve the overall fault tolerance and flexibility of the system,
eliminating the \roo as a potential bottleneck.
However, running consensus elections across large systems is inefficient in the best of
cases, and prohibitively slow in a geo-replicated environment.
Root quorum decision-making is kept tractable by having replicas \emph{delegate} their
votes, usually to their local leaders, for a finite duration.
With leader delegation, the root membership effectively consists of the set of \sub
leaders in the ideal case.
Because \subs themselves are geographically distributed and placed to optimize system
specific policies, the \roo represents a simplified snapshot of the overall operation
and as the network environment and access patterns change, so too does the \roo.
To simplify the discussion of delegated voting, we assume a leader-oriented consensus
protocol that operates similarly to Raft, using heartbeats and timeouts to detect
failure.

The \roo elects a leader for a root term, $r$ and broadcasts this term along with
routine heartbeats that keep the leader term alive.
Each replica has a delegated term, $d$, on root heartbeat, if $r>d$, then the replica
sets $d=r$ and marks its vote as no longer delegated.
If the replica is not delegated it can issue a delegate request to local replicas for a
delegate term, $d_t=d+j$ where $j$ defines how many root elections the delegation will
survive.
Usually, $j=1$, but can be increased if intermittent failure in the root quorum is
expected (to allow the delegates to smoothly transition to new leaders without
re-delegation).
When a delegate request is received, the replica can delegate iff $d_t>d$ and marks
itself as delegated, otherwise it reports the current term to the delegate candidate.
A \emph{delegate} is any replica that is able to vote in root term $r$, e.g. any replica
whose $d>r$.

During a consensus operation in the \roo for either a root leader election or an epoch
change, the root leader or candidate broadcasts a request to \emph{all replicas} with
its current term.
Replicas only reply to the vote if $d \ge r$ and they have not delegated their vote.
We consider it possible, depending on the implementation, that duplicate votes might be
received, therefore to ensure safety, the root leader or candidate must account for
which replicas have voted, keeping only the votes for the highest delegated term.
This is true even for hot spares, which are not currently in any subquorum.
Delegates reply with the unique ids of the replicas they represent so that root
consensus decisions are still made using a majority of all system replicas.
This is correct because vote requests now reach all replicas, and because replicas whose
votes have been delegated merely ignore the request.
We argue that it is also efficient, as a commit's efficiency depends only on receipt of
a majority of the votes.

Delegation ensures that root quorum membership is always the entire system and remains
unchanged over subquorum leader elections and even reconfiguration.
Delegation is essentially a way to optimistically shortcut contacting every replica for
each decision.
Large consensus groups are generally slow, not just because of communication latency,
but because large groups in a heterogeneous setting are more likely to include replicas
on very slow hosts or networks.
In the usual case for our protocol, the root leader still only needs to wait for votes
from the subquorum leaders.
Leaders are generally those that respond more quickly to timeouts, so the speed of root
quorum operations is unchanged.
Note that because the root term only increments when the root leader changes,
epoch decisions are the commonly delegated votes.

Delegation can also be seen as a \texttt{PRE-PREPARE} phase before the root leader
election (e.g. the root quorum \texttt{PREPARE}).
If a root election timeout expires, \textbf{only a delegate can become a root candidate}
(including replicas that vote only for themselves).
This means that at least one root election will occur with delegation, prompting
re-delegation requests that are generally far in advance of new elections. In normal
operation, with limited failure, all root decisions are delegated.
However, if the delegates representing a majority of the system fail, so long as there
is at least one live delegate, the root term will increase until all delegations are
busted, causing all replicas to re-delgate their votes to a live replica, or to simply
vote for themselves.
There is one critical edge case, all delegates failing simultaneously, which we discuss
in Section~\ref{section:assassination}.

The optimal \roo configuration is to have all \sub leaders as delegates with hot spares
delegated evenly to local delegates.
This configuration balances the fault tolerance of the \roo with the throughput of
decision making, however this configuration is not guaranteed by the protocol described
above.
Our approach employs heuristic mechanisms such as network distance or maximum delegation
to ensure the efficiency of delegated voting and to limit delegation inside of single
regions only.
The \roo leader can simplify this process by identifying replicas that have previously
been highly available members of the root quorum.
If the \roo leader identifies a poor delegation state, it simply initiates a root leader
election for itself, incrementing $r$, and causing replicas to re-delegate after the
election.
On epoch change, the root leader can hint to the reconfigured \subs the best replicas
to delegate to if they are not already delegated for the epoch.
If no hints are provided, then replica followers generally delegate their vote to the
term 1 leader and hot spares to the closest subquorum leader.
Generally, the reconfigurations that occur during epoch changes allow the \roo to move
the system to an optimal state given current network conditions.

\subsection{Reconfiguration}
\label{section:reconfiguration}

% this section could probably be expanded a lot for example:
% what happens if a \sub is partitioned? Can a reconfiguration be issued?

Every epoch represents a new configuration of the system as designated
by the root leader.
Efficient reconfiguration ensures that the system is both dynamic,
responding both to failures and changing usage patterns, and minimizes
coordination by colocating related objects.
An epoch change is initiated by the root leader in response to one of
several events, including:

\begin{itemize}
    % not sure if the below actually works as described ... delegated joint consensus
    % may be required -- leaving out to be safe rather than sorry.
    % \item notification of join requests by new replicas
    \item notification of failed replicas
    \item a namespace repartition request to optimize data accesses or minimize conflicts
    \item changing network conditions that suggest re-assignment to improve \sub throughput
    \item application initiated reconfigurations to localize data, increase durability, or enforce policies
\end{itemize}

The root leader transitions to a new epoch through the normal commit
phase in the \roo.
The command proposed by the leader is an enumeration of the new subquorum
partition, namespace partition, and assignment of namespace portions to
specific subquorums.
The announcement may also include initial leaders for each subquorum,
with the usual rules for leader election applying otherwise, or if the
assigned leader is unresponsive.
Upon commit, the operation serves as an \emph{announcement} to subquorum
leaders.
Subquorum leaders repeat the announcement locally, disseminating full
knowledge of the new system configuration, and eventually transition to
the new epoch by committing an \texttt{epoch-change} operation locally.

The epoch change is lightweight for subquorums that are not directly
affected by the overarching reconfiguration.
If a subquorum is being changed or dissolved, however, the
\emph{epoch-change} commitment becomes a tombstone written to the logs
of all local replicas.
No further operations will be committed by that version of the subgroup,
and the local shared log is archived and then truncated.
Truncation is necessary to guarantee a consistent view of the log within
a subquorum, as peers may have been part of different subquorums, and
thus have different logs, during the last epoch.
Replicas then begin participating in their new subquorum instantiation.
In the common case where a subquorum's membership remains unchanged
across the transition, an \texttt{epoch-change} may still require
additional mechanism because of changes in namespace responsibility.

\subsection{Data Placement}
\label{section:placement}

% NOTE: we have a figure for data placement that is not used for space.
% this section could also use a lot of expansion.

Reconfiguration by the \roo not only allows the system to transparently scale consensus
operations, but also provides applications the ability to directly specify how and where
consensus operations should be placed.
Although epoch changes are relatively heavy weight, the design of \hc allows them to be
routine.
Most systems are optimized for only one type of data placement, however real-world
applications vary in their needs for availability and durability, often on a per-object
basis.
Applications may also observe markedly different access patterns that change during the
course of operation, including:

\begin{itemize}
    \item single ownership: objects are accessed in one region and do not migrate.
    \item revolving access: objects are accessed from a primary region that changes over time.
    \item conflicting access: objects are continuously accessed simultaneously from multiple regions.
\end{itemize}

% probably should be 2 paragraphs but concatenated for space reasons
These access patterns imply not only the optimal placement of replicas in specific
geographic regions, but also the consensus protocol that should be used to maximize
throughput.
For example, if commit latency for significant numbers of accesses to a small set of
objects outweighs durability requirements, a \sub can be placed in a single region using
Raft, which serializes all accesses through a single leader and is immune to conflicting
reads and writes.
To increase durability, data can be placed with a leader and primary backup in a single
region, and a secondary backup in a remote region (it is important with this scheme to
modify the election rules of Raft to decrease the probability that the secondary backup
is elected leader).
To handle conflicting accesses across regions, EPaxos or Mencius can be used to
optimistically serialize proposals across the wide area.
\Hc requires no special modifications to \sub operation, only requiring that \subs
intersect with the \roo and that delegation and transitions are implemented correctly.
As a result, the \roo can specify not only the placement of namespace shards with the
replica, but also the consensus algorithm that governs them.
We envision the \roo as a distributed system management framework that is not only able
to apply policy changes efficiently, but also to monitor real-time performance and to
make adaptations on demand.
Adaptations are the key to the protocols flexibility but also ensure that the system is
lightweight enough to be managed at a global scale without significant resources.

\subsection{Fuzzy Transitions}
\label{section:fuzzy}

% this section absolutely needs to be expanded with hand-off details including:
% - tombstones
% - retries and forwarding
% - what if the transitioning \sub is not available?
% - where does data reside and where does it go?
% - optimistic replication of data objects to optimize transitions

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/namespaceHandoff}
    \caption{Fuzzy transitions allow the system to safely make progress unimpeded by
    leadership changes in either the \roo or \sub.}
    \label{fig:handoff}
\end{figure}

Epoch handshakes are required whenever the namespace-to-\sub mapping changes across an
epoch boundary.
HC separates epoch transition announcements in the \roo from implementation in \subs.
Epoch transitions are termed \emph{fuzzy} because \subs need not all transition
synchronously.
There are many reasons why a \sub might be slow.
Communication delays and partitions might delay notification or temporary failures might
block local commits.
A \sub might also delay transitioning to allow a local burst of activity to
cease such as currently running transactions\footnote{The HC protocol
discussed in this paper does not currently support transactions.}.
Safety is guaranteed by tracking \sub dependencies across the epoch boundary.

Figure~\ref{fig:handoff} shows an epoch transition where the scopes of
$q_i$, $q_j$, and $q_k$ change across the transition to epoch $e$ as follows:

\begin{align*}
  \label{eq:3}
  q_{i,e-1} = n_a, n_b  &\longrightarrow q_{i,e} = n_a\\
  q_{j,e-1} = n_c, n_d  &\longrightarrow q_{j,e} = n_c,n_d,n_f\\
  q_{k,e-1} = n_e, n_f  &\longrightarrow q_{k,e} = n_d,n_e
\end{align*}

All three \subs learn of the epoch change at the same time, but become ready
with varying delays.
These delays could be because of network lags or ongoing local activity.
\Sub $q_i$ gains no new objects across the transition and moves immediately to the new
epoch.
\Sub $q_j$'s readiness is slower, but then it sends requests to the
owners of both the new objects it acquires in the new epoch.
Though $q_i$ responds immediately, $q_k$ delays its response until locally
operations conclude.
Once both handshakes are received, $q_j$ moves into the new epoch, and $q_k$
later follows suit.

These bilateral handshakes allow an epoch change to be implemented
incrementally, eliminating the need for lockstep synchronization across the entire
system.
This flexibility is key to coping with partitions and varying connectivity in
the wide area.
However, this piecewise transition, in combination with \sub re-definition and
configuration at epoch changes, also means that individual replicas \emph{may
be part of multiple \subs at a time}.

This overlap is possible because replicas may be mapped to distinct subgroups
from one epoch to the next.
Consider $q_k$ in Figure~\ref{fig:handoff} again.
A single replica process, $p$, may be remapped from \sub $q_{k,e-1}$ to \sub $q_{i,e}$
across the transition.
\Sub $q_{k,e-1}$ is late to transition, but $q_{i,e}$ begins the new epoch
almost immediately.
Requiring $p$ to participate in a single \sub at a time would potentially delay
$q_{i,e}$'s transition and impose artificial synchronicity constraints on the
system.
One of the many changes we made in the base Raft protocol is to allow a replica to have
multiple distinct shared logs.
Smaller changes concern the mapping of requests and responses to the appropriate
consensus group.

\section{Fault Tolerance}
\label{section:ft}

We assert that consensus at the leaf replicas is correct and safe because decisions are
implemented using well-known leader-oriented consensus approaches.
Hierarchical consensus therefore has to demonstrate linearizable correctness and safety
between \subs for a single epoch and between epochs.
Briefly, linearizability requires external observers to view operations to objects as
instantaneous events.
Within an epoch, subquorum leaders serially order local accesses, thereby guaranteeing
linearizability for all replicas in that quorum.
% Remote accesses and the internal invariant also enforce linearizability of accesses
% between \subs.

Epoch transitions raise the possibility of portions of the namespace being re-assigned from one \sub to
another, with each \sub making the transition independently.
Correctness is guaranteed by an invariant requiring \subs to delay serving newly
acquired portions of the namespace until after completing all appropriate handshakes.

\begin{table*}[t]
  \centering
  \begin{tabular}{l|l} \hline
    \mcn{Failure Type} & \mcn{Response} \\ \hline
    \sub peer & request replica repartition from \roo \\
    \sub leader & local election, request replacement from \roo \\
    root leader & root election (with delegations)\\
    majority of majority of \subs & (nuclear option) root election after delegations
                                    timed out \\
  \end{tabular}
  \label{tab:categories}
\end{table*}

\subsection{Failures}
\label{section:failure}

During failure-free execution, the \roo partitions the system into
disjoint \subs, assigns \emph{\sub leaders}, and assigns partitions
of the tagspace to \subs.
Each \sub coordinates and responds to accesses for objects in its assigned
tagspace.
We define the system's \emph{safety} property as guaranteeing that
non-linearizable (or non-sequentially-consistent)
event orderings can never be observed.
We define the system's \emph{progress} property as the system having enough
live replicas to commit votes or operations in the \roo.

The system can suffer several types of failures, as shown in
Table~\ref{tab:categories}.
Failures of \sub and \roo leaders are handled through the normal consensus
mechanisms.
Failures of \sub peers are handled by the local leader petitioning the \roo to
re-configure the \sub in the next epoch.
Failure of a \roo peer is the failure of \sub leader, which is handled as
above.
\Roo heartbeats help inform other replicas of leadership changes, potentially
necessary when individual \subs break down.

HC's structure means that some faults are more important than others.
Proper operation of the \roo requires the majority of replicas in the majority of \subs to
be non-faulty.
Given a system with $2m+1$ \subs, each of $2n+1$ replicas, the entire
system's progress can be halted with as few as $(m+1)(n+1)$ well-chosen
failures.
Therefore, in worst case, the system can only tolerate:
\begin{align*}
f_{worst}=mn+m+n
\end{align*}
failures and still make progress.
At maximum, HC's basic protocol can tolerate up to:
% f_{max} = (m+1)*n + m*(2n+1) = mn + n + 2mn+m = 3mn+m+n
\begin{align*}
f_{best} = (m+1)*n + m*(2n+1) = 3mn+m+n
\end{align*}
failures.
As an example, a 25/5 system can tolerate at least 8 and
up to 16 failures out of 25 total replicas.
A 21/3 system can tolerate at least 7, and a maximum of 12,
failures out of 21 total replicas.
Individual \subs might
still be able to perform local operations despite an impasse at the global level.

Total \sub failure can temporarily cause a portion of the namespace to be unserved. However, the \roo
eventually times out and moves into a new epoch with that portion assigned to another
\sub.

\subsection{Assassination}
\label{section:assassination}

Singleton consensus protocols, including Raft, can tolerate just under half of the entire system
failing.
As described above, HC's structure makes it more vulnerable to clustered failures.
Therefore we define a \emph{nuclear option}, which uses direct consensus
decision among all system replicas to tolerate any $f$ replicas
failing out of $2f+1$ total replicas in the system.

A nuclear vote is triggered by the failure of a root leader election.
A \emph{nuclear candidate}
increment's its term for the \roo and broadcasts a request for votes to all
system replicas.
The key difficulty is in preventing delegated votes and
nuclear votes from reaching conflicting decisions.
Such situations might occur when temporarily unavailable \sub leaders regain connectivity
and allow a wedged \roo to unblock.
Meanwhile, a nuclear vote might be concurrently underway.

Replica delegations are defined as intervals over specific slots.
Using local \sub slots would fall prey to the above problem, so we define
delegations as a small number (often one) of root slots, which usually
correspond to distinct epochs.
During failure-free operation, peers delegate to their leaders and are all
represented in the next root election or commit.
Peers then renew their delegations to their leaders by appending them to the
next local commit reply.
This approach works for replicas that change \subs over an epoch
boundary, and even allows peers to delegate their votes to arbitrary other
peers in the system (see replicas $r_N$ and $r_O$ in Figure~\ref{fig:system}).

This approach is simple and correct, but deals poorly with leader turnovers in
the \subs.
Consider a \sub where all peers have delegated votes to their leader
for the next root slot.
If that leader fails, none of the peers will be represented.
We finesse this issue by re-defining such delegations to count
root elections, root commits, \emph{and} root heartbeats.
The latter means that local peers will regain their votes for the next \roo
action if it happens after to the next heartbeat.

Consider the worst-case failure situation: a majority of the majority of \subs have
failed.
None of the failed \sub leaders can be replaced, as none of those \subs have
enough local peers.

The first response is initiated when a replica holding delegations (or its own
vote) times out waiting for the root heartbeat.
That replica increments its own root term, adopts the prior system
configuration as its own, and becomes a root candidate.
This candidacy fails, as a majority of \sub leaders, with all of their
delegated votes, are gone.
Progress is not made until delegations time out.
In our default case where a delegation is for a single root event, this
happens after the first root election failure.

At the next timeout, any replica might become a candidate because delegations have
lapsed (under our default assumptions above).
Such a \emph{nuclear} candidate increments its root term and
sends candidate requests to all system replicas,
succeeding if it gathers a majority across all live replicas.

The first candidacy assumed the prior system configuration in its candidacy
announcement.
This configuration is no longer appropriate unless some of the ``failed''
replicas quickly regain connectivity.
Before the replica announces its candidacy for a second time, however, many of
the replica replies have timed out.
The candidate alters its second proposed configuration by recasting all such
replicas as hot spares and potentially reducing the number and size of the
subgroups.
Subsequent epoch changes might re-integrate the new hot spares if the replicas
regain connectivity.

\section{Implementation}
\label{section:implementation}

\pjk{description of protocol and implementation should be discrete}

Alia is implemented with Raft

Only versions are replicated, object data is stored in blobs and replicated using
eventual consistency techniques.

2 phase thrifty messaging: once to leaders then to followers

\section{Evaluation}
\label{section:evaluation}

HC was designed to adapt both to dynamic workloads as well as variable network
conditions.
We therefore evaluate HC in three distinct environments: a homogeneous data center, a
heterogeneous real-world network, and a globally distributed cloud network.
The homogeneous cluster is hosted on Amazon EC2 and includes 26 ``t2.medium'' instances:
dual-core virtual machines running in a single VPC with inter-machine latencies
($\lambda$) normally distributed with a mean, $\lambda_{\mu}=0.399ms$ and standard
deviation, $\lambda_{\sigma}=0.216ms$.
The heterogeneous cluster (UMD) consists of several local machines distributed across a
wide area, with inter-machine latencies ranging from
$\lambda_{\mu}=2.527ms$,
$\lambda_{\sigma}=1.147ms$ to $\lambda_{\mu}=34.651ms$,
$\lambda_{\sigma}=37.915ms$.
The variability of this network also poses challenges that HC is uniquely suited to
handle via root quorum-guided adaptation.
We explore two distinct scenarios -- sawtooth and repartitioning -- using this cluster;
all other experiments were run on the EC2 cluster.

In our final experiment, we explore the use of hierarchical consensus in an extremely
large, planetary-scale system comprised of 105 replicas in 15 data centers in 5
continents spanning the northern hemisphere and South America. This experiment was also
hosted on EC2 ``t2.medium`` instances in each of the regions available to us at the time
of this writing. In this context, reporting average latencies is difficult as
inter-region latencies depend more on network distance than can be meaningfully
ascribed to a single central tendency.

\subsection{Basic Performance}
\label{section:performance}

\begin{figure*}[t]
    \centering
    \minipage{0.5\textwidth}
        \includegraphics[width=\linewidth]{figures/scaling.pdf}
        \caption{Throughput increases with larger quorum sizes.}
        \label{fig:scaling_consensus}
    \endminipage\hfill
    \minipage{0.5\textwidth}%
        \includegraphics[width=\linewidth]{figures/hc_throughput_workload.pdf}
        \caption{Alia handles larger workloads with larger system sizes.}
        \label{fig:throughput_workload}
    \endminipage
\end{figure*}

HC is partially motivated by the need to scale strong consistency to large cluster sizes.
We based our work on the assumption that consensus performance decreases as the quorum
size increases, which we confirm empirically in Figure~\ref{fig:scaling_consensus}.
This figure shows the maximum throughput against system size for a variety of workloads,
up to 120 concurrent clients.
A workload consists of one or more clients continuously sending writes of a specific
object or objects to the cluster without pause.

Standard consensus algorithms, Raft in particular, scale poorly with uniformly
decreasing throughput as nodes are added to the cluster.
Commit latency increases with quorum size as the system has to wait for more responses
from peers, thereby decreasing overall throughput.
Figures~\ref{fig:scaling_consensus} and~\ref{fig:throughput_workload}
clearly show the multiplicative advantage of HC's hierarchical structure.
Note that though HC is not shown to scale linearly in these figures, this is due to
performance bottlenecks of the networking implementation in these experiments.
In our final experiment, we show linear scaling with our latest implementation of HC.

There are at least two factors limiting the HC throughput shown in our initial experiments.
First, the HC subquorums for the larger system sizes are not saturated.
A single 3-node subquorum saturates at around 25 clients and this experiment has only
about 15 clients per subquorum for the largest cluster size.
We ran experiments with 600 clients, saturating all subquorums even in the 24-node case.
This throughput peaked at slightly over 50,000 committed writes per second, better but
still lower than the linear scaling we had expected.

We think the reason for this ceiling is hinted at by Figure~\ref{fig:throughput_workload}.
This figure shows increasingly larger variability with increasing system sizes.
A more thorough examination of the data shows widely varying performance across
individual subquorums in the larger configurations.
After instrumenting the experiments to diagnose the problem, we determined it was a bug
in the networking code, which we repaired and improved.
By aggregating append entries messages from clients while consensus messages were
in-flight, we managed to dramatically increase the performance of single quorums and
reduce the number of messages sent.
This change also had the effect of ensuring that the variability was decreased in our
final experiment.

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/ec2_latency_cumfreq.pdf}
    \caption{Cumulative latency of requests decreases with system size.}
    \label{fig:latency}
\end{figure}

The effect of saturation is also demonstrated in Figure~\ref{fig:latency}, which shows
cumulative latency distributions for different system sizes holding the workload
(number of concurrent clients) constant.
The fastest (24/3) shows nearly 80\% of client write requests being serviced in under
2 msec.
Larger system sizes are faster because the smaller systems suffer from contention (25
clients can saturate a single subquorum).
Because throughput is directly related to commit latency, throughput variability can be
mitigated by adding additional subquorums to balance load.

\subsection{Adaptability}
\label{section:adaptability}

\begin{figure*}[t]
    \centering
    \minipage{0.5\textwidth}
        \includegraphics[width=\linewidth]{figures/umd_fault_tolerance.pdf}
        \caption{Reconfiguration to adapt to changing access patterns.}
        \label{fig:fault_tolerance}
    \endminipage\hfill
    \minipage{0.5\textwidth}%
        \includegraphics[width=\linewidth]{figures/umd_sawtooth.pdf}
        \caption{Reconfiguration to take over from failing subquorums.}
        \label{fig:sawtooth}
    \endminipage
\end{figure*}

          \pjk{What heuristics/parameterization allows the system to
            adapt in sawtooth? If we have to, go back and derive a
            heuristic from what was actually done (was it all just
            scripted or is there a heuristic?)}

Besides pure performance and scaling, HC is also motivated by the need to adapt to
varying environmental conditions.
In the next set of experiments, we explore two common runtime scenarios that motivate
adaptation: shifting client workloads and failures.
We show that HC is able to adapt and recover with little loss in performance. These
scenarios are shown in Figures~\ref{fig:sawtooth} and
\ref{fig:fault_tolerance} as throughput over time, where vertical dotted
lines indicate an epoch change.

The first scenario, described by the time series in Figure~\ref{fig:fault_tolerance}
shows an HC 3-replica configuration moving through two epoch changes.
Each epoch change is triggered by the need to localize tags accessed by
clients to nearby subquorums.
% The experiment was run over machines with widely varying latency.
The scenario shown starts with all clients co-located with the subquorum serving the tag
they are accessing.
However, clients incrementally change their access patterns first to a tag located on
one remote subquorum, and then to the tag owned by the other.
In both cases, the root quorum adapts the system by repartitioning the tagspace such
that the tag defining their current focus is served by the co-located subquorum.

Figure~\ref{fig:fault_tolerance} shows a 3-subquorum configuration where one
entire subquorum becomes partitioned from the others.
After a timeout, the root uses an epoch change to re-allocate the tag of the partitioned
subquorum over the two remaining subquorums.
The partitioned subquorum eventually has an heuristic \emph{obligation timeout}, after
which the  root quorum is not obliged to leave the tag with the current subquorum.
The tag may then be re-assigned to any other subquorum.
Timeouts are structured such that by the time an obligation timeout fires, the root
quorum has already re-mapped that subquorum's tag to other subquorums.
As a result, the system is able to recover from the partition as fast as possible.
In this figure, the repartition occurs through two epoch changes, the first allocating
part of the tagspace to the first subquorum, and the second allocating the rest of the
tag to the other.
Gaps in the graph are periods where the subquorums are electing local leaders.
This may be optimized by having leadership assigned or maintained through root consensus.

\subsection{Planet Scale Consensus}
\label{section:scaling}

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/geoconsensus_throughput}
    \caption{Consensus scales linearly as the number of replicas increases.}
    \label{fig:geoconsensus_throughput}
\end{figure}

In our final implementation we ran our repaired version of HC at a planetary scale.
We created a system with 105 replicas in 15 regions in 5 continents.
The system allocated size 3 subquorums round-robin to each region such that the largest
system was comprised of 6 subquorums per region with 1 hot-spare per region.
Figure~\ref{fig:geoconsensus_throughput} shows the global blast throughput of
the system, the sum of throughput of client process that fired off 1000 concurrent
requests, timing the complete response.
To mitigate the effect of global latency, each region ran independent blast clients to
its local subquorums, forwarding to remote quorums where necessary.
To ensure that the system was fully throttled during the throughput experiment, we timed
the clients to execute simultaneously using the AWS Time Sync service to ensure that
clocks were within 100 nanoseconds of each other.
In these results we show that our HC implementation does indeed scale linearly.
Adding more nodes to the system increases the fault tolerance (e.g. by allocating hot s
pares) if enough nodes are added to add another subquorum, the capacity of the system to
handle client requests is also increased.

\section{Related Work}
\label{section:related}

% HC is a protocol for composing and coordinating consensus groups, maintaining
% consistency invariants over large systems, and adapt to changing conditions and
% application loads.

The principal contribution of this paper, \hc, follows from the
large body of work on improving throughput in distributed consensus
over the Paxos protocol~\cite{paxos_simple,epaxos,flexible_paxos,generalized_paxos},
and on Raft~\cite{raft,raft_refloated}, which focus primarily on fast vs.
slow path consensus, eliding phases with dependency resolution, and load
balancing.

Our work is orthogonal to these in that \subs and the \roo can be
implemented with different underlying consensus algorithms, though the
two levels must be integrated quite tightly.
Further, HC abstracts reconfiguration away from \sub consensus, allowing
multiple \subs to move into new configurations and reducing the need
for joint consensus~\cite{raft} and other heavyweight procedures.
Finally, its hierarchical nature allows the system to multiplex multiple
consensus instances on disjoint partitions of the object space while
still maintaining global consistency guarantees.

The global consistency guarantees of HC are in direct contrast to other
systems that scale by exploiting multiple consensus
instances~\cite{mdcc,spanner} on a per-object basis.
These systems retain the advantage of small quorum sizes but cannot provide
system-wide consistency invariants.
Another set of systems uses quorum-based decision-making but relaxes
consistency guarantees~\cite{dynamo,pnuts}; others provide no way to
pivot the entire system to a new configuration~\cite{scatter}.
Chain replication~\cite{chain_replication} and Vertical Paxos~\cite{vertical_paxos,niobe}
are among approaches that control Paxos instances through other consensus decisions.
However, HC differs in the deep integration of the two different levels.
Whereas these approaches are top down, HC consensus decisions at the root
level replace system configuration at the \sub level, and vice versa.

Possibly the closest system to HC is Scatter~\cite{scatter}, which uses an
overlay to organize consistent groups into a ring.
Neighbors can join, split, and talk amongst themselves.
The bottom-up approach potentially allows scaling to many
\subs, but the lack of central control makes it hard to implement global
re-maps beyond the reach of local neighbors.
HC ties \roo and \subs tightly together, allowing \roo decisions to
completely reconfigure the running system on the fly either on demand or
by detecting changes in network conditions.

Recent work has similarly explored a more strategic approach to data placement;
Akkio~\cite{akkio}, for instance, optimizes shard placement using access
latency as the cost function.
HC goes a step further, offering not optimization but rather fine grain control
over data placement, which allows applications to determine their own
use-case specific cost functions and heuristics.

We claim very strong consistency across a large distributed system, similar
to Spanner~\cite{spanner}.
Spanner provides linearizable  transactions through use of special hardware
and environments, which are used to tightly synchronize clocks in the
distributed setting.
Spanner therefore relies on a very specific, curated environment.
HC targets a wider range of systems that require cost effective scaling in
the data center to rich dynamic environments with heterogeneity on all levels.

Finally, shared logs have proven useful in a number of settings from fault
tolerance to correctness guarantees.
However, keeping such logs consistent in even a single consensus instance has
proven difficult~\cite{chubby,zookeeper}.
More recent systems are leveraging hardware support to provide fast access to
shared logs~\cite{vcorfu,tango,fawn}.
To our knowledge, HC is the first work to propose synchronizing shared logs
across multiple discrete consensus instances in the wide area.


\section{Discussion}
\label{section:discussion}

Alia takes a different approach to implementing geo-distributed systems, focusing
on a system's ability to be \emph{flexible}.
Flexibility ensures that the system can balance requirements for throughput and
availability while still maintaining the strongest possible consistency semantics.
To achieve this, Alia is based on three primary design requirements that inform the
rest of the framework.

\emph{Requirement 1: Systems should be as fluid as the information they contain.}
Many systems are optimistic, they assume that conflict is rare and that objects are
accessed in standard patterns that change.
In our experience, both people and information flows freely therefore a system must
accommodate organic and shifting usage patterns; for example a set of objects may
primarily be accessed only in daylight, requiring the system to adapt by moving the
coordinating replicas to the locales currently in working hours.
To accommodate this requirement, Alia is designed to regularly and safely transition
through reconfigurations called epoch changes, reallocating replicas into subquorums
to manage specific partitions of the namespace.
Epoch changes are \emph{fuzzy} to ensure that reconfiguration does not need to be
synchronous and hand-offs are optimized through anti-entropy replication of data.

\emph{Requirement 2: No partial failures.}
A system's size should be its advantage -- allowing increased throughput with linear
scaling, and better placement to optimize accesses.
Often, however, a system's size increases its complexity and it's susceptibility
to unique failures such as correlated cascading failure.

Alia is designed with a single process model -- the same process participating in
the root quorum also handles messages for the subquorum(s) the process has been
assigned to.
This model ensures that if a replica fails it cannot participate in some decision
making, such as configuration, but not others, such as accesses.
This requirement also allows us to more easily tackle complex failures; such as
using a nuclear option (discussed in 5.3) to ensure progress even with a worst-case
failure of delegates, or ensuring that leases are either respected or replaced
for whole subquorums that fall out of communication.

\emph{Requirement 3: Consistency semantics must be transparent and interpretable.}
As privacy and security become increasingly important requirements of distributed
systems, consistency is no longer about ensuring that your boss cannot see your
Spring Break pictures on a social network wall.
Instead, consistency is about ensuring that the correct operations are being
executed on the correct replicas and that data can be audited to discover its
exact placement.
Alia ensures that there is an intersection between subquorums where data accesses
are taking place and the root quorum where configuration and namespace partitions
are occurring.
This intersection is optimized by delegated voting to ensure that the root
quorum can make progress and remain fluid.
The intersection also guarantees that a complete, externalizable log of events
for the global system can be exported on demand.

% broadly: fluidity, adaptivity, transparency, bigness
% another item about how size is an advantage to leverage rather than a problem to cope with, an opportunity to scale without adding complications...

% need segue to HC section

% Unused content for Agile Systems subsection
% Wants:
% - Ability to easily modify participants in the system.
% - Detectable failure modes.
% - Responsiveness to changing access patterns
% - Recoverability and ease of maintenance
% - Strong consistency and transactions
% There exists many processes each of which are running in an availability zone in multiple regions.
% Read-write accesses happen to objects according to geographic patterns - single region, multiregion, etc.
% System wants to replicate these accesses to maintain a minimum amount of durability, e.g. protect from a zone failure; but it's not total replication.


\section{Conclusion}
\label{section:conclusion}

The next generation of distributed systems will be geographically replicated around the
planet in order to provide better performance by preventing bottlenecks and localizing
accesses to international and highly mobile users and to provide durability in the face
of catastrophic failure.
We have presented \hc, an implementation and extension of Vertical Paxos, that is
designed to scale coordination and transparently provide strong consistency in order to
build and deploy systems that span globe.
\Hc is a framework of intersecting tiers of quorums whose primary benefit is flexibility,
which allows large systems to dynamically adapt to changing conditions, improving both
performance and maintainability.

\Hc handles challenges of geo-distributed consensus through flexible reconfiguration.
Increasing network distance between replicas increases latency and the probability of
network partitions, making strong consistency a challenge.
To handle this, we separate the concerns of placement and access decisions to the \roo
and \subs, allowing as much of the system to operate as independently as possible.
Centralized administration is impossible in a global context, so the \roo is able to
adapt the system automatically by observing conditions and applying policy-driven
changes to the system in real time with fuzzy transitions.
To ensure correct reasoning of global consistency semantics and reduce the complexity of
independent-subsystems with different failure modes, \hc ensures that there is an
intersection of the \roo and subs.
This intersection requires all nodes to participate in the root quorum, so to scale
this quorum, we introduce delegated voting to improve globally availability.
Finally, because objects have different requirements for availability or durability,
data placement rules and subquorum behavior can be adjusted for different geographic
access patterns.

% Future work:
% - investigate the affect of more tiers of consensus
% - transactions
% - automatic adaptability using learning rather than policies and heuristics


% TODO: Why are years being omitted from the references?!
\bibliographystyle{plain}
\bibliography{papers}

\end{document}
