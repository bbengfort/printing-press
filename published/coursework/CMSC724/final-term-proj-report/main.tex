\documentclass[11pt,letterpaper]{article}

\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage[titletoc,toc,title]{appendix}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{listings,url}
\usepackage[parfill]{parskip}

\pagestyle{fancyplain}
\fancyhf{}
\fancyfoot[R]{\footnotesize Page \thepage\ of \pageref{LastPage}}

\renewcommand{\headrulewidth}{0.0pt} % No header rule
\renewcommand{\footrulewidth}{0.4pt} % Thin xfooter rule

\begin{document}

\title{Graph-Based Machine Learning on Relational Data}

\author[ ]{Konstantinos Xirogiannopoulos}
\author[ ]{Benjamin Bengfort}
\affil[ ]{Department of Computer Science}
\affil[ ]{University of Maryland}
\affil[ ]{\textit{\{kostasx,bengfort\}@cs.umd.edu}}

\date{May 11, 2015}

\maketitle

% Introduction %

\section*{Introduction}

Graph-based analyses are becomming increasingly popular, particularly for large scale machine learning upon sparsely connected, highly related data sets. Graph data structures inherently exist in many different data sets from finance to health care and learning mechanisms to extract and discover structure, communication patterns, and vital nodes allow insights beyond traditional information extraction techniques. Moreover, graph data structures lend themselves well to applications like large scale search, aggregation, and transformation tasks by allowing groups of nodes to be operated on via edge traversal. In this context, traversal is a substitute for iteration which minimizes the amount of processing required starting with some initial set of vertices \cite{berretti_efficient_2001}. This efficiency has led to the development of parallel, large scale graph computation  engines and more recently, native graph databases.

% In the real paper we should probably get to the point a tad quicker.

The rise of graph databases has been directly related to the challenge associated with the coordination of logical-graph organization and requirements for physical storage. Graph data management techniques have focused primarily on the ease of analysis. Graph traversals and computations can be more easily expressed in a graph-specific query language like Gremlin \cite{rodriguez_gremlin_2013} or Cypher \cite{miller_graph_2013} rather than a declarative or low level programming language like SQL or PG/PSQL \cite{rodriguez_exploring_2012}. The expressiveness of these query languages enable the implementation of lightweight, efficient machine learning algorithms. By forcing data into a network model, the index-free adjacency and other graph-specific properties allow for effective analyses like centrality, shortest path, community detection, and more to be computed in real time. For larger graphs, vertex-centric computation \cite{malewicz_pregel:_2010} has shown that graph algorithms scale well and can be leveraged not only in transactional data systems, but also in distributed computation.

While graph algorithms can be computed locally for a small group of nodes \cite{jadhav_comparative_2014} or at scale across extremely large networks for population-wide analyses \cite{cuzzocrea_big_2014}, the base data set is usually too large to fit into memory, thereby requiring some form of data storage and management - preferably a system that provides consistency and durability guarentees as well as some transactional system. Current methodologies directly relate to the NoSQL movement or to parallel online anlaytical (OLAP) processing rather than transactional interaction with graphs in real time. Although the implementation of graph-specific techniques enhances graph analysis, native graph data management and storage has not caught up; a situation made clear by the many tensions expressed in a wide array of current research, discussed in the related works section.

Relational Database Management Systems (RDBMS), on the other hand, have been well studied and optimized in their longer history. More mature RDBMS systems are performant as transactional systems where few records are selected, inserted or updated at transaction time. Although this model doesn't lend itself to larger scale analytics, RDBMS systems play vital operational and theoretical roles in the larger database space. Many studies have concluded that relational databases, while not specifically designed for graphs, are still tools that should be extended to provide solutions for graph-based algorithms \cite{welc_graph_2013,najork_hammers_2012,vicknair_comparison_2010}.

% Nice paragraph below!

In this paper, we outline our work towards creating a system for efficient in-memory graph analytics using relational data. Our ongoing work has mainly focused on defining some initial specifications of our declarative Datalog-based DSL, and various optimizations. The optimizations that we have implemented facilitate a more efficient extraction of the graph from the relational schema -- both in terms of time complexity and memory usage. For our implementation, we used Java and the popular BluePrints library to expose an API familiar to many users. The features of our system in its current version are the following:\\

\begin{enumerate}

	\item Defining initial specifications for our declarative DSL, and writing a parser and translator to SQL.
	\item Expanding the functionality of the DSL towards and optimizing extraction for ego-networks.
	\item Minimizing the memory footprint of the extracted graph via a \textit{compressed representation}
	\item Exploring the space of space efficient graph representations and implementing our custom CSR-like structure. %compressed Report

\end{enumerate}

Based on our experimental results, we believe that research to integrate a realational data store and graph analytics will continue to provide rich insights both into machine learning techniques but into database technology in general.

\section*{Datalog as a facilitating language to SQL}

The primary critique for using relational databases in graph computation is that the primary programming abstraction, SQL, is declarative and not well suited to the description of graph algorithms. Graph-specific query languages like Gremlin \cite{rodriguez_gremlin_2013}, Cypher \cite{miller_graph_2013}, and SPARQL \cite{prudhommeaux_sparql_2008} are able to easily express traversals and computations required for graph algorithms. Because these DSLs are directly related to the graph computational model, their expressive power and execution context are fundamentally different than that of the traditional iterator-model of relational databases. For example, Gremlin exposes a pipeline-iterative mechanism of computation and SPARQL can be directly translated into lambda calculus.

As a result, there has been interest in the use of Datalog, a declarative programming language, to extend relational algebra into the graph algebra domain \cite{he_graphs-at--time:_2008}. When graph computations can be expressed as a query operation selection operators can be generalized to pattern matching and composition operators can be included to rewrite graphs. Moreover, Datalog is a syntactic subset of Prolog, therefore it can also be used as a query language for deductive databases. Recent advances have adapted Datalog with recursion and extended Datalog specifically for graphs \cite{shkapsky_graph_2013,green_datalog_2013}, allowing for aggregation, negation, and typing - monotonic constraints that are fundamentally a part of a relational database.

The conversion of Datalog queries to SQL queries, thereby extending relational processing with graph aware processing is not just isolated to recursion on a single machine, but has also been shown to be parallelizable \cite{seo_distributed_2013}. Datalog has also been shown to be easily extensible to large scale machine learning on big data sets \cite{bu_scaling_2012}.

Datalog is clearly a fascinating choice for research as it has been shown to be complete and effective at graph computation, usable on a relational database, and parallelizable. Previous work has focused on the adaption to new graph domains, but there has been no complete system where Datalog augments a relational database in a specific fashion for both parallel and efficient graph computation.

\subsection*{DSL Definition and Implementation}

\begin{figure}[t]
\centering
\scriptsize
\begin{lstlisting}[breaklines,basicstyle=\ttfamily]
(1) Nodes(ID, Name) :- Author(ID, Name)
    Edges(ID1, ID2) :- AuthorPub(ID1, PubID),
                       AuthorPub(ID2, PubID)
(2) For Author(X, _).
      Nodes(ID, Name) :- Author(X, Name).
      Nodes(ID, Name) :- AuthorPub(X,P), AuthorPub(ID,P),
                   Author(ID, Name)
      Edges(ID1, ID2) :- Nodes(ID1, _), Nodes(ID2, _),
                   AuthorPub(ID1, P), AuthorPub(ID2, P)
\end{lstlisting}
\vspace{-10pt}
\caption{Graph Extraction Query Examples}
\vspace{-10pt}
\label{fig:queries}
\end{figure}

A core piece in the implementation of our project is the complete and correct implementation of a translation module which converts a graph-sepcific Datalog DSL to a SQL-based query plan, in order to perform relational operations. In the majority of cases, there is a very straightforward mapping between the two languages. However, no open source implementation of the translator was readily available. We were therefore required to write our own implementation.

The translation of Datalog to SQL centers around the process of mapping predicate symbols of \textit{atoms} (predicate symbols along with their list of arguments or terms) to database tables and terms to columns within those tables. The actual name of the term in the queries does not need to directly mirror the database; it can simply be an alias. The key idea behind mapping is that the ordering of arguments reflects the order of the columns in the database table. Each alias is therefore directly mapped to a projection of the column that appears in that position in the table. The left-side terms also must appear either in the right side, or somewhere else in the query as variables - an important technique that will be further discussed in the next sections. A balancing of terms is necessary to ensure the left side does not represent an existent predicate (table), but that the view will be generated using right-side predicates.

In our initial implementation, the translator was achieved using a \texttt{Lexer}, which parses the DSL using \textit{regular expressions}, returning an \texttt{Iterable} of the individual tokens which make up the query. Each token is tagged with a specific label depending on its position in the query. For example, in Query 1 of Figure \ref{fig:queries}, \texttt{Author} and \texttt{AuthorPub} are names of relations, so the arguments will be treated as the aliases of the adjacent columns in those exact positions in the underlying table. Because the user is assumed to have full access to and knowledge of the database's schema, we leave the left and right side balancing as a necessary user provided restriction.

\subsection*{Extending Datalog for Ego-Graphs}

In order to define an expressive DSL for graphs, we explored the types of operators that are necessary to expanding the relational domain in a meaningful way, such that operators are meaningful and intuitive. Our current approach is to extend Datalog to support an iterative \texttt{For} loop. This loop functionality facilitates the extraction of a specified \textit{set} of multiple graphs rather than a single graph. For example, $N$-hop \textit{ego-networks} are very interesting for a variety of tasks and analyses and can be extracted from multiple graphs. Given a single node (the ego), an $N$-hop ego network is the neighborhood of the ego, as well as the neighborhoods of the neighbors up to $N$ links away from the center. To extract this subgraph from multiple graphs, some form of recursion is required.

The \texttt{For} loop we have implemented is showcased in Query 2 of Figure \ref{fig:queries}. In this query the loop iterates through every \texttt{Author.Id} (the \texttt{Id} is the first column) in the \texttt{Author} table, and computes the graph specified in the indented queries below, which includes all Authors who have published with author \texttt{X} in the nodes, as well as edges between those authors, defined the same way. This query will extract the 1-hop neighborhood for author \texttt{X}, and will repeat for each value of \texttt{X} in the database.

\subsection*{Optimizing the Extraction of Ego-Graphs}

Under normal execution, our system translates our Datalog DSL into standard SQL and executes the appropriate queries against PostgreSQL. Unfortunately, when using the \texttt{For} loop to specify more than one graph, this operation makes continuous queries to the database, receiving independent results -- a hugely expensive operation in terms of time and memory. For instance, if a user wanted to extract and apply analytics on the entire range of ego-graphs within a table, the amount of queries made against the database would equal the number of distinct IDs. We have found that there is substantial overhead associated with every distinct query to the database, whose effects become limiting even in small datasets.

In order to minimize computational time, we have attempted to reduce the amount of computation required by PostgreSQL, moving work to our software, thereby reducing the need for constant queries to the database. In particular, this tactic optimizes the extraction of an entire range of ego-graphs. We have developed a mechanism which requires only a single query to PostgreSQL in order extract all of the necessary information for creating all specified, in-memory graphs.

This mechanism uses a \texttt{union of queries} operation in order to fully use a single SQL query for obtaining all edges in the entire graph. This single large query contains any and all of edges required for obtaining ego-graphs, and is therefore sufficient without further calls to the data store. By using the result set from the Edges query as a view via a \texttt{WITH} query, we were able to apply a \textit{tag} to each tuple of the result set such that the aggregation of tags signfies an ego-graph in which the specific edge participates. An example of a query which is used to achieve a similar result is described in Figure \ref{fig:tagging}. This query returns a result set where each edge is tagged with the ego-graphs the vertices particpate in, and allows for the construction of 1-hop neighborhoods for each Node in the range of the query.

In order to construct the graphs after completing these steps, the only processing required is a single pass through that result-set. We then create each ego-graph, as we go along, adding edge after edge as we come across it, to the adjacent ego-graph that is specified in the aggregated array of tags for that edge. The performance gains in applying this optimizations appear to be substantial. For the aforementioned subset of the DBLP dataset, the total time to extract the neighborhoods of all nodes in the co-authors graph came down from 18s to approximately 2.2s using this optimization.

\begin{figure}[t]
\scriptsize
\begin{lstlisting}[breaklines,basicstyle=\ttfamily]
WITH Edges AS (<SQL Query for the Edges>),
Result AS
(
SELECT DISTINCT e1.X AS e1u, e1.id2 AS e1v, X as TAG
FROM Edges e1
WHERE e1.X IN (SELECT DISTINCT X FROM Edges)

UNION

SELECT DISTINCT e2.X, e2.id2,e1.X AS TAG
FROM Edges e1,Edges e2
WHERE e1.X IN (SELECT DISTINCT X FROM Edges) AND e1.id2=e2.X
)
SELECT e1u,e1v,array_agg(tag) FROM Result GROUP BY e1u,e1v;
\end{lstlisting}
\vspace{-10pt}
\caption{Result-Set Tagging for Efficient Ego-Graph Extraction}
\vspace{-10pt}
\label{fig:tagging}
\end{figure}


\subsection*{Compressing the Output Graph}

%resultset  can get large
After the queries in our DSL are translated to SQL applying the above optimizations when necessairy, the next step is to generate and load the graph into memory. The size of the result-set after the produced SQL is executed against the database, depends on two factors; the current size of the participating tables in the database, and the amout of predicates in the query that could potentially narrow down the size of the result-set returned. To eliminate  the latencies associated with moving data back and forth from disk to memory, the entire graph is required to reside in memory for the entirety of the computation. Another goal we aimed for was thus to decrease the memory footprint of the generated graphs by our system, in a lossless fashion that would continue to allow vertex centric programs to run against the graph.

%we did it with this high overview idea
The way we accomplished this is by using a \textit{compressed representation} of the graph. Methods like this one been studied in the past \cite{feder_1995}, and revolve around the idea of compressing the number of edges in highly condensed cliques, to a minimum, by having each of the nodes in the clique connect to only a single \textit{virtual} node instead.

%this is why we did it
In most cases, graphs are comprised of nodes of the same \textit{type}, being connected to each-other in various ways. In order to materialize these connections in the form of edges, a \textit{self join} is usually required. Self-joins are simply join operations where we are joining a table with itself, in order to find the intersections between two of its own columns, based on some predicate. When the size of the table starts getting reasonably large however, these self-joins become increasingly slow to complete, quickly reaching a point where the query is essentially impossible to run in a reasonable amount of time.

%this is how we did it
Our solution to this problem is that instead of pushing the self-join to PostgreSQL, we instead delay the self-join and apply it directly to the final graph. In essence, this means the actual join operation will only be computed gradually, and only when the computation requires a node to send a message to its neighbors. The implementation of this path is quite simple. With a single projection of the table that needs to be self-joined, and a sequential scan over it, we create a \textit{virtual} node for every distinct destination vertex, (where the destination vertex is the join condition for the self-join) and serially attatch nodes to their adjacent virtual nodes accordingly.

%this is what it gives us
Many large real-world graphs can contain a substantial amounts of very densely connected cliques, where many nodes are all connected to each-other via seperate, distinct edges. This representation will hence allow for substantial amounts of compression. As an example, going back to the aforementioned DBLP example dataset, for a publication with k authors, the co-authors graph contains $k(k-1)/2$ edges between those authors. By using the compressed representation, the number of edges per clique get compressed to the minimum of $k$ edges (just one for every one of the $k$ nodes), with the (usually) small trade-off of one extra node. The benefits of using such a representation of the graph are therefore two-fold: by amortizing the latency of  self-joins, it allows for extracting larger graphs from very large relational tables, that were otherwise impossible to extract by actually computing the self-join, and that they naturally result in graphs with a significantly smaller memory footprint, thus enabling analytics on larger graphs. This representation is currently as a subclass on top of \texttt{TinkerGraph}, which entails many limitations which we intend to move away from in the future versions.

%Graph Processing on Relational Datastores

\subsection*{Moving away from TrinkerGraph}

In the implementation of the system thus far, the underlying graph structure has been based on \texttt{TinkerGraph}. TinkerGraph is the default graph implementation of the widely used BluePrints API (which is part if the Tinkerpop \cite{tinkerpop} stack) and it serves as an example of how one should go about when implementing the Graph interfaces of Blueprints.
As we know, there is a variety of ways a graph can be represented in memory. Some of these ways are optimized edge or vertex traversal, some are implemented for optimal iteration of the neighboring vertices. In the same ways, some are designed with space efficiency in mind, and some are not. The approach we are taking considering our ideas for the condensed representation dictate that our graph needs to be space optimal since  the primary purpose of the condensed representation is for the graph to take up as small amounts of space as possible, and thus give users the ability to load larger graphs for in-memory processing.

As such, even though TinkerGraph provides a complete implementation of the property graph, it is certainly not built with space efficiency in mind. One of the reasons this is true is because it stores Edges as seperate explicit objects in order to adhere to fully supporting the entirety of BluePrints API.

In search of the most efficient graph representation for our purposes, we looked into what other in-memory graph computation frameworks like Ligra \cite{ligra}, Green-Marl \cite{greenmarl}, Galois \cite{galois} have chosen to use before us, as their goals are very similar to ours.  Each of these frameworks is striving to minimize the amount of space usage to allow as large graphs as possible to be loaded and computed upon. As it turns out, every single one of these systems for the most part utilizes the \textit{CSR} (Compressed Sparse Row) representation. CSR is essentially an adjacency matrix, that is compressed in the sense that it does not store the zero values since sparse matices can get very space inefficient. While CSR seems to be optimal when it comes to space, it flaws in the fact that it is an \textit{immutable} structure. (There has been recent work in exploring ways to make this representation mutable by means of \textit{snapshots} in LLAMA \cite{llama}, however we have not further exploited this path.)

The immutability of CSR constitutes a problem in our case, because we want to be able to construct the graph from the result-set we obtain from querying the database (PostgreSQL) and load the graph to its final format at the same time without needing to use other data structures only to copy everything back into a CSR at the end of the loading phase.

As pointed out however in \cite{usman2006performance}, the CSR representation does not have significantly different performance than by using an adjacency list representation with the same concepts in mind. The Adjacency List would keep all of the in and out vertices, of the graph and would have the same exact performance. The only difference is that Adjacency lists provide capability for the efficient addition of new vertices (and inefficiently support edge removal), which is perfect for our case because for purposes of loading the graph. We ended up coding up three simple graph representations, where the fundamental differences were only the way that the \textit{in} and \textit{out} neighbors were stored for each vertex. We tried the following apporaches:
\begin{enumerate}
	\item Using singly linked linked lists of single vertices.
	\item A scheme of using a linked list of \textit{arrays} of vertices. This is the most sophisticated scheme, where the size of the array that resides within each linked-list grows \textit{exponentially}, until reaching a specific value (in our case 16), to optimize the cost of storing the empty values in each array.
	\item Using Java ArrayLists in a CSR flavor.
\end{enumerate}

\subsection*{Experiments}
For the purposes of choosing the best representation, we conducted a few simple experiments to test the performance of these three different representations. We used the \textit{cast\_info} table (~30M rows) in the imdb database in order to load the condensed graph of actors, where an actor is connected with another actor if they've played together in the same movie. In order for this graph to be constructed normally, join of the \textit{cast\_info} with itself is need, which would normally be impossible. We set the java a heap space to maximize at 12GB, and our finding are listed in Table \ref{tab:results}.

\begin{table}[h!]
	\centering
	\begin{tabular}{| l | c | c |}
		\hline
		Method & Space & Time \\
		\hline
		Using Linked List & 8.3GB & 65 seconds \\
		Using Exponentially Growing Array & 9.99GB & 100 seconds \\
		Using CSR ArrayLists & 8.04GB & 64 seconds \\
		Using TinkerGraph & (?) & Never Completes \\
		\hline
	\end{tabular}
	\caption{Experimental results for graph construction.}
	\label{tab:results}
\end{table}


There is a (relatively small) overhead  associated with storing the pointers of a linked list, versus using ArrayLists. Our most surprising finding was that the ArrayList approach did not take longer than the LinkedList approach, even though one would expect it to, considering the fact that re-sizing an ArrayList requires copying over the entirety of the elements to the new ArrayList. Finally, the performance of our custom implementation of an exponencially growing array, was both slower and less memory efficient than the other two simpler methods, which is something that we need to further explore.

\subsection*{Related Work}

Since the advent of specialized systems, there has been some criticism from the relational community that a specific-framework isn't required. The primary line of thought is that relational databases can perform competitive or superior performance for most implementations and that the benefit of specialization is far outweighed by the cost of replacing an existing infrastructure \cite{welc_graph_2013}.

There is a fair amount of past and ongoing work on using relational datastores for graph computation. This work mostly focuses on doing so by means of SQL queries against the database. Most of these systems support and expand on the notion that there may be no real need for the existence of specialized graph computation frameworks, and attempt to prove this by means of creating these systems.

Grail \cite{fan_case_2015} is a syntactic layer for querying a graph in a vertex-centric way such that the graph is stored in an relational database management system
\cite{fan_case_2015}. The authors of Grail propose a query language that can then be translated into SQL and executed upon relational tables of vertices and edges. The authors hope to show that a relational datastore is both productive and performant in graph computations and that a specialized system is not necessary. They also lay the foundation for considering the construction of relational devices like indices and tables for graph-specific querying on existing data.

Vertexica \cite{jindal_vertexica:_2014} also provides a graph analysis toolkit on top of a relational database, similarly injecting data storage, query processing, and query interface support to the relational system for vertex-centric graph analysis \cite{jindal_vertexica:_2014}. Vertexica also uses a vertex/edge table for querying data, provides per-vertex transactions, and uses indices for fast retrieval of message passing. Vertexica also introduces many interesting graph-specific optimization techniques that are reminiscent of the columnar/relational database that the system sits on top of. In particular, they implement table unions to avoid expensive three way joins, parallel workers, batching, and late materialization.

Finally the Trinity \cite{shao_trinity:_2013} database, while not on a traditional relational store, implements a distributed graph engine on a memory cloud (a key-value store that is loaded into memory) for low latency graph queries \cite{shao_trinity:_2013}. Trinity, similar to Grail and Vertexica, implements a graph query layer called TSL that allows users to declare a schema and vertex-centric computation that is translated to the storage layer. Trinity's storage layer is strikingly similar to the aforementioned systems, and the distributed hash table (DHT) for vertices seems to be implemented primarily for distributed storage and fast access by the Trinity cluster. Potentially a sharded relational cluster would serve the same requirement.

It is therefore apparent that the main goals of this work are very different from the aforementioned systems. Our goal is to provide a layer of abstraction that will allow users to apply in-memorty graph analytics on their already existent relational data, without assuming that their data is already in graph format (tables of Vertices and Edges), in contrast to these related works.

\section*{Discussion and Future Work}

Because specialized systems for parallel graph processing and native graph computation exist, any system that is researched or developed must be at least comparable to the specialized systems. Two considerations for the system are related to the non-relational techniques:

\begin{enumerate}
	\item ACID transactions on local subsets of vertices and edges
	\item Fast, parallel offline analyses of large portions of the database
\end{enumerate}

Specialized systems focus on storage layers that optimize their ability to lock vertices and their edges in a contiguous fashion without blocking other transactions. Parallel graph computation frameworks focus on two primary methods: vertex-centric computation and gather-analyze-sort (GAS) models. Relational databases have been researched extensively to provide both transaction isolation and serializability as well as parallel forms of computation, hopefully we can build upon that research to provide operation comparable to specialized systems.

It is clear that the programming interface to graph computations on a relational database must be graph specific, SQL is declarative but not expressive enough for most types of iterative or recursive computations. Datalog appears to be an extremely good candidate, as work has been done to ensure that Datalog is able to easily operate on graphs or be translated to a relational structure.

Conversion techniques on top of existing relational stores indicate that graphs may be induced or inferred from existing structures with user intervention. This seems to indicate there is no need for predefined vertex, edge, and message tables. However, these tables might be useful as non-persistent in-memory indices or data structures to optimize or improve computation.

Based on what we've learned from our previous survey \textit{A Survey of Graph Analyses using Databases}, our work has so far focused on the Flexible extraction of a graph from an existing data store into memory for computation (reducing the impact of join operations), and will continue by potentially exploring:

\begin{enumerate}
	\item Explore optimizations and generalization of the Compressed Representation.
	\item Use of existing relational tools (indices, stored procedures, operators) to improve graph processing in a meaningful way.
	\item Extensible graph inference and creation from the schema of a relational database and exposure to these graphs from a graph-specific query language.
\end{enumerate}

Therefore while our proposal has some similarities to the Datalog papers, the conversion papers, and the three systems that produce graph operators on top of relational operators - we intended to produce a complete system that is both fast and efficient for graph processing on top of existing, native relational storage.

\section*{Conclusion}

Graph analyses and computations are important for efficient and large scale machine learning and local queries and traversals. Since much data is easily expressible using a graph model there has been interest in the use of graph algorithms and processing frameworks for production systems. The trend has been to produce \textit{specialized} graph computation engines from native graph databases that use NoSQL techniques to parallel graph processing frameworks that do not depend on any particular data store.

This interest and research has shown that it is easier to express computation programatically using a light-weight graph specific query language. As a result, the relational database community has shown that with a minimal amount of translation the logical interface of an RDBMS can easily be adapted to graph processing with no change to the physical dependencies. Not only that, several studies have shown that relational databases are competitive with new solutions, primarily as a result of their maturity.

This paper has also explored workflows for the conversion and ETL of graphs out of and into relational systems, the use of Datalog as a graph query language, and direct comparisons between graph databases and relational ones. This culminated with a specific look at three systems that are implementing graph processing on a relational store: Grail, Vertexica, and Trinity - though by the apriori creation of vertex, edge, and message tables to facilitate computation.

We have made substantial progress towards integrating these ideas into a single working system that uses a graph-specific query language translated to relational algebra, inferring existing graphs from the schema.

\bibliographystyle{plain}
\bibliography{paper}

\end{document}
