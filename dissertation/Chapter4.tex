%Chapter 4

\renewcommand{\thechapter}{4}

\chapter{Federated Consistency}
\label{ch:federated_consistency}
% Overview, details, consistency, performance, lessons learned.

Hybrid consistency model

\section{Overview}

\section{Eventual Consistency}

\todo{Read/Write Quorums}

Clients can \texttt{Put} (write) and \texttt{Get} (read) key-value pairs to
 and from one or more replicas in a single operation.
The set of replicas that responds to a client creates a quorum that must
agree on the state of the operation at its conclusion.
Clients can vary read and write quorum sizes to improve consistency or
availability -- larger quorums reduce the likelihood of inconsistencies
caused by concurrent updates, but smaller quorums respond much more quickly,
particularly if the replicas in the quorum are co-located with the client.
In large, geo-replicated systems we assume that clients will prefer to choose
fewer, local replicas to connect with, optimistic that collisions across the
wide-area are rare, e.g. that writes are localized but reads are global.

On \texttt{Put}, the instance of the key-value pair created by the update is
assigned a monotonically increasing, conflict-free \textit{version
number}~\cite{version_conflict_detection,version_vectors}.
For simplicity, we assume a fixed number of replicas, therefore each version
is made up of two components: the \textit{update} and \textit{precedence ids}.
Precedence ids are assigned to replicas during configuration, and update ids
are incremented to the largest observed value during synchronization.
As a result, any two versions generated by a \texttt{Put} anywhere in the
system are comparable such that the \textit{latest} version of the key-value
pair is the version with the largest update id, and in the case of ties, the
largest precedence id.

Additional version metadata, including the parent version of the update (in a
read-then-write system or simply the latest version of the key stored
locally), implements a virtual object history that allows us to reason about
consistency.
Keys can be managed independently, e.g. each key has its own update id
sequence resulting in per-object consistency, or all objects can be managed
together with a single sequence; in the latter case, it is possible to
construct an ordering history of operations to all objects and in the former,
a sequence of operations for each object.
Object histories allow us to reason about the global consistency of the
system.

There are two primary inconsistencies that can occur in this system:
\textit{stale reads} and \textit{forked writes}.
A stale read means that the \texttt{Get} operation has not returned
the globally most recent version of the object, e.g. the local replica is
behind in the object history.
A forked write is caused when there are two concurrent writes to the same
object, a symptom of stale reads.
Forked writes cause a divergence in the object history such that there are
two or more branches of update operations.
As we will see in the next section, one of these writes will eventually be
\textit{stomped} before it can become fully replicated, meaning that the
eventual consistency prunes these branches at the cost of losing
the update.
The ideal consistency for a system is represented by a linear object history
without forks~\cite{rethinking_eventual}, which demonstrates that the
system was in a consistent state during all accesses.

% In chapter 6
% Both forms of inconsistency can be primarily attributed to \emph{visibility
% latency}, that is the time it takes for an update to propagate to all
% replicas in the system.
% Visibility latency is directly related to the likelihood of stale reads with
% respect to the frequency of accesses~\cite{quantifying_pbs}; said
% another way, decreasing the visibility latency improves the overall
% consistency of a system.
% However, in a system that uses anti-entropy for replication, the propagation
% speed of an update is not governed solely by network connections, it is also
% bound to the number and frequency of anti-entropy sessions conducted as well
% as the radius of the network.

\todo{Anti-Entropy Sessions and Synchronization}

Anti-entropy sessions are conducted in a pairwise fashion on a periodic
interval to ensure that the network is not saturated with synchronization
requests which may reduce client availability.
At each interval, every replica selects a synchronization partner such that
all replicas have a uniform likelihood of selection.
This ensures that an update originating at one replica will be propagated to
all online replicas given the continued operation of replication.
This mechanism also provides robustness in the face of failure; a single
unresponsive replica or even network partition does not become a bottleneck
to synchronization, and once the failure is repaired synchronization will
occur without reconfiguration.

There are two basic forms of synchronization: \textit{push} synchronization
is a fire-and-forget form of synchronization where the remote replica is sent
the latest version of all objects, whereas \textit{pull} synchronization
requests the latest version of objects and minimizes the size of data
transfer.
To get the benefit of both, we consider \textit{bilateral} synchronization
which combines push and pull in a two-phase exchange.
Bilateral synchronization increases the effect of anti-entropy during each
exchange because it ensures that in the common case each replica is
synchronized with two other replicas instead of one during every anti-entropy
period.

Bilateral anti-entropy starts with the initiating replica sending a vector of
the latest local versions of all keys currently stored, usually optimized
with Merkel or prefix trees to make comparisons faster.
The remote replica compares the versions sent by the initiating replica with
its current state and responds with any objects whose version is
\textit{later} than the initiating replica's as well as another version
vector of requested objects that are earlier on the remote.
The initiating replica then replies with the remote's requested objects,
completing the synchronization.
We refer to the first stage of requesting later objects from the remote as
the pull phase, and the second stage of responding to the remote the push
phase.

There are two important things to note about this form of anti-entropy
exchange.
First, this type of synchronization implements a \textit{latest writer wins}
policy.
This means that not all versions are guaranteed to become fully replicated
-- if a later version is written during propagation of an earlier version,
then the earlier version gets \emph{stomped} by the later version because
only the latest versions of objects are exchanged.
If there are two concurrent writes, only one write will become fully
replicated, the write on the replica with the greater precedence.

% IN Chapter 6 
% Second, visibility latency is maximized when all replicas choose a remote
% synchronization partner that does not yet have the update.
% This means that maximal visibility latency is equal to $t\log_3n$, where
% $t$ is the anti-entropy interval and $n$ is the number of replicas in the
% network.
% In practice, however, because of inefficient exchanges due to uniform random
% selection of synchronization partners, this latency is never practically
% achieved, and is instead modulated by a noise variable that is
% proportional to the size of the network.

Policies: latest writer wins

Bilateral anti-entropy

\subsection{Consistency Failures}

Forks

Stale Reads

\section{Integration}

\subsection{Communication Integration}

\subsection{Consistency Integration}

- Forte Number

\section{Performance Evaluation}

Communication Topology
Inconsistencies due to outages
Inconsistency due to system latency
