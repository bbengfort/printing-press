%Chapter 7

\renewcommand{\thechapter}{7}

\chapter{Related Work}
\label{ch:related_work}

Spanner~\cite{spanner} provides global consistency by sharding each tablet across multiple Paxos groups then externalizes their consistency using TrueTime, delaying the commit until a window of uncertainty has passed.

CalvinFS~\cite{calvindb,calvinfs} batches transaction operations across the wide area, but still requires paxos to be deployed across the wide area.

Systems that implement many small quorums of
coordination~\cite{mdcc,scatter,spanner} avoid the centralization bottleneck
and reliability concerns of master-service
systems~\cite{gray_dangers_1996,gfs} but create silos of independent
operation that are not coordinated with respect to each other.

We have theorized that cloud services present the opportunity for deploying data services on a trusted infrastructure. If it is not trusted, however, we can use an encryption model similar to SPORC to combine multiple cloud providers into a single data system~\cite{sporc}. 

\section{Hierarchical Consensus}

Our principle contribution is Hierarchical Consensus, a general technique to compose consensus groups, maintain consistency invariants over large systems, and adapt to changing conditions and application loads.
HC is related to the large body of work improving throughput in distributed consensus over the Paxos protocol~\cite{paxos,epaxos,fexible_paxos,generalized_paxos}, and on Raft~\cite{raft,raft_refloated}.
These approaches focus on fast vs. slow path consensus, eliding phases with dependency resolution, and load balancing.

Our work is also orthogonal in that subquorums and the root quorums can be implemented with different underlying protocols, though the two levels must be integrated quite tightly.
Further, HC abstracts reconfiguration away from subquorum consensus, allowing multiple subquorums to move into new configurations and reducing the need for joint consensus~\cite{raft} and other heavyweight procedures.
Finally, its hierarchical nature allows the system to multiplex multiple consensus instances on disjoint partitions of the object space while still maintaining global consistency guarantees.

The global consistency guarantees of HC are in direct contrast to other systems that scale by exploiting multiple consensus instances~\cite{bigtable,mdcc,spanner} on a per-object basis.
These systems retain the advantage of small quorum sizes but cannot provide system-wide consistency invariants.
Another set of systems uses quorum-based decision-making but relaxes consistency guarantees~\cite{dynamo,pnuts,cops}; others provide no way to pivot the entire system to a new configuration~\cite{scatter}.
Chain replication~\cite{van2004chain} and Vertical Paxos~\cite{vertical_paxos} are among approaches that control Paxos instances through other consensus decisions.
However, HC differs in the deep integration of the two different levels.
Whereas these approaches are top down, HC consensus decisions at the root level replace system configuration at the subquorum level, and vice versa.

Possibly the closest system to HC is Scatter~\cite{scatter}, which uses an overlay to organize consistent groups into a ring.
Neighbors can join, split, and talk amongst themselves. The bottom-up approach potentially allows scaling to many subquorums, but the lack of central control makes it hard to implement global re-maps beyond the reach of local neighbors.
HC ties the root quorum and subquorums tightly together, allowing root quorum decisions to completely reconfigure the running system on the fly either on demand or by detecting changes in network conditions.

We claim very strong consistency across a large distributed system, similar to Spanner~\cite{spanner}.
Spanner provides linearizable  transactions through use of special hardware and environments, which are used to tightly synchronize clocks in the distributed setting.
Spanner therefore relies on a very specific, curated environment. HC targets a wider range of systems that require cost effective scaling in the data center to rich dynamic environments with heterogeneity on all levels.

Finally, shared logs have proven useful in a number of settings from fault tolerance to correctness guarantees.
However, keeping such logs consistent in even a single consensus instance has proven difficult~\cite{chubby,gfs,zookeeper}.
More recent systems are leveraging hardware support to provide fast access to shared logs~\cite{vcorfu,tango,calvindb,calvinfs,hyder-a,fawn}.
To our knowledge, HC is the first work to propose synchronizing shared logs across multiple discrete consensus instances in the wide area.
